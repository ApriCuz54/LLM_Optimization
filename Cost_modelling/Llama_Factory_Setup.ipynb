{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installing Dependencies"
      ],
      "metadata": {
        "id": "4wlS8G4P8jO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "%rm -rf LLaMA-Factory\n",
        "!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
        "%cd LLaMA-Factory\n",
        "%ls\n",
        "!pip install -e .[torch,bitsandbytes]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "vdwlf0cZ8hlL",
        "outputId": "4b9c7e73-5ced-4b20-927d-b3e8b7e6248b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'LLaMA-Factory'...\n",
            "remote: Enumerating objects: 362, done.\u001b[K\n",
            "remote: Counting objects: 100% (362/362), done.\u001b[K\n",
            "remote: Compressing objects: 100% (276/276), done.\u001b[K\n",
            "remote: Total 362 (delta 79), reused 314 (delta 72), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (362/362), 9.95 MiB | 23.48 MiB/s, done.\n",
            "Resolving deltas: 100% (79/79), done.\n",
            "/content/LLaMA-Factory\n",
            "\u001b[0m\u001b[01;34massets\u001b[0m/       \u001b[01;34mevaluation\u001b[0m/  MANIFEST.in     requirements.txt  \u001b[01;34mtests\u001b[0m/\n",
            "CITATION.cff  \u001b[01;34mexamples\u001b[0m/    pyproject.toml  \u001b[01;34mscripts\u001b[0m/\n",
            "\u001b[01;34mdata\u001b[0m/         LICENSE      README.md       setup.py\n",
            "\u001b[01;34mdocker\u001b[0m/       Makefile     README_zh.md    \u001b[01;34msrc\u001b[0m/\n",
            "Obtaining file:///content/LLaMA-Factory\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers!=4.52.0,<=4.52.4,>=4.49.0 (from llamafactory==0.9.4.dev0)\n",
            "  Downloading transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting datasets<=3.6.0,>=2.16.0 (from llamafactory==0.9.4.dev0)\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting accelerate<=1.7.0,>=1.3.0 (from llamafactory==0.9.4.dev0)\n",
            "  Downloading accelerate-1.7.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting peft<=0.15.2,>=0.14.0 (from llamafactory==0.9.4.dev0)\n",
            "  Downloading peft-0.15.2-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting trl<=0.9.6,>=0.8.6 (from llamafactory==0.9.4.dev0)\n",
            "  Downloading trl-0.9.6-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting tokenizers<=0.21.1,>=0.19.0 (from llamafactory==0.9.4.dev0)\n",
            "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: gradio<=5.31.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.4.dev0) (5.31.0)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.4.dev0) (3.10.0)\n",
            "Collecting tyro<0.9.0 (from llamafactory==0.9.4.dev0)\n",
            "  Downloading tyro-0.8.14-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.4.dev0) (0.8.1)\n",
            "Collecting numpy<2.0.0 (from llamafactory==0.9.4.dev0)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.4.dev0) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.4.dev0) (1.15.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.4.dev0) (0.2.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.4.dev0) (0.9.0)\n",
            "Collecting modelscope>=1.14.0 (from llamafactory==0.9.4.dev0)\n",
            "  Downloading modelscope-1.28.0-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: hf-transfer in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.4.dev0) (0.1.9)\n",
            "Collecting fire (from llamafactory==0.9.4.dev0)\n",
            "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.4.dev0) (2.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.4.dev0) (24.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.4.dev0) (5.29.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.4.dev0) (6.0.2)\n",
            "Collecting pydantic<=2.10.6 (from llamafactory==0.9.4.dev0)\n",
            "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.4.dev0) (0.35.0)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.4.dev0) (0.115.14)\n",
            "Collecting sse-starlette (from llamafactory==0.9.4.dev0)\n",
            "  Downloading sse_starlette-2.4.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting av (from llamafactory==0.9.4.dev0)\n",
            "  Downloading av-15.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.4.dev0) (0.11.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.4.dev0) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.15.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.4.dev0) (0.21.0+cu124)\n",
            "Collecting bitsandbytes>=0.39.0 (from llamafactory==0.9.4.dev0)\n",
            "  Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.7.0,>=1.3.0->llamafactory==0.9.4.dev0) (5.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.7.0,>=1.3.0->llamafactory==0.9.4.dev0) (0.33.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.7.0,>=1.3.0->llamafactory==0.9.4.dev0) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.4.dev0) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.4.dev0) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.4.dev0) (0.3.7)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.4.dev0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.4.dev0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.4.dev0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.4.dev0) (0.70.15)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.4.dev0)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (4.9.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.6.0)\n",
            "Requirement already satisfied: gradio-client==1.10.1 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (1.10.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (3.10.18)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (11.2.1)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.12.2)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (4.14.1)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (15.0.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.4.dev0) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.4.dev0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.4.dev0) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.4.dev0) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.4.dev0) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.4.dev0) (2.9.0.post0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from modelscope>=1.14.0->llamafactory==0.9.4.dev0) (75.2.0)\n",
            "Requirement already satisfied: urllib3>=1.26 in /usr/local/lib/python3.11/dist-packages (from modelscope>=1.14.0->llamafactory==0.9.4.dev0) (2.4.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->llamafactory==0.9.4.dev0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->llamafactory==0.9.4.dev0) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<=2.10.6->llamafactory==0.9.4.dev0) (0.7.0)\n",
            "Collecting pydantic-core==2.27.2 (from pydantic<=2.10.6->llamafactory==0.9.4.dev0)\n",
            "  Downloading pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (3.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->llamafactory==0.9.4.dev0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->llamafactory==0.9.4.dev0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->llamafactory==0.9.4.dev0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->llamafactory==0.9.4.dev0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->llamafactory==0.9.4.dev0)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->llamafactory==0.9.4.dev0)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->llamafactory==0.9.4.dev0)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->llamafactory==0.9.4.dev0)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->llamafactory==0.9.4.dev0)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->llamafactory==0.9.4.dev0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->llamafactory==0.9.4.dev0) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.52.0,<=4.52.4,>=4.49.0->llamafactory==0.9.4.dev0) (2024.11.6)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.11/dist-packages (from tyro<0.9.0->llamafactory==0.9.4.dev0) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro<0.9.0->llamafactory==0.9.4.dev0) (13.9.4)\n",
            "Collecting shtab>=1.5.6 (from tyro<0.9.0->llamafactory==0.9.4.dev0)\n",
            "  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn->llamafactory==0.9.4.dev0) (8.2.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn->llamafactory==0.9.4.dev0) (0.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->llamafactory==0.9.4.dev0) (3.1.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.4.dev0) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.4.dev0) (0.60.0)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.4.dev0) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.4.dev0) (1.5.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.4.dev0) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.4.dev0) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.4.dev0) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.4.dev0) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.4.dev0) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.4.dev0) (1.1.1)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from omegaconf->llamafactory==0.9.4.dev0) (4.9.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (1.3.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.4.dev0) (3.11.15)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (1.0.9)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate<=1.7.0,>=1.3.0->llamafactory==0.9.4.dev0) (1.1.5)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa->llamafactory==0.9.4.dev0) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa->llamafactory==0.9.4.dev0) (4.3.8)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.0->llamafactory==0.9.4.dev0) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.4.dev0) (3.4.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.4.dev0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.4.dev0) (2.19.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa->llamafactory==0.9.4.dev0) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa->llamafactory==0.9.4.dev0) (1.17.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.4.dev0) (1.5.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.4.dev0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.4.dev0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.4.dev0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.4.dev0) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.4.dev0) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.4.dev0) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.4.dev0) (1.20.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->llamafactory==0.9.4.dev0) (2.22)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.4.dev0) (0.1.2)\n",
            "Downloading accelerate-1.7.0-py3-none-any.whl (362 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.1/362.1 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl (72.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading modelscope-1.28.0-py3-none-any.whl (5.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m118.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m102.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.15.2-py3-none-any.whl (411 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.1/411.1 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.7/431.7 kB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m94.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m108.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m94.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.9.6-py3-none-any.whl (245 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-0.8.14-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.8/109.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading av-15.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (39.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.7/39.7 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sse_starlette-2.4.1-py3-none-any.whl (10 kB)\n",
            "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shtab-1.7.2-py3-none-any.whl (14 kB)\n",
            "Building wheels for collected packages: llamafactory, fire\n",
            "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llamafactory: filename=llamafactory-0.9.4.dev0-0.editable-py3-none-any.whl size=27716 sha256=0f5bda12e0ef6e2c19d712bb8cfec53cdefb3bc8978243a884cdf76bec2dfcb7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-07vt27ai/wheels/bd/34/05/1e3cb4b8f20c20631b411dc5157b4b150850c03496fa96c2c4\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=3442047ccadb607ab1beb8eba91773e264e7a46c328db8f2f6e004a6af4346b0\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\n",
            "Successfully built llamafactory fire\n",
            "Installing collected packages: shtab, pydantic-core, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, fsspec, fire, av, sse-starlette, pydantic, nvidia-cusparse-cu12, nvidia-cudnn-cu12, modelscope, tyro, tokenizers, nvidia-cusolver-cu12, transformers, datasets, bitsandbytes, accelerate, trl, peft, llamafactory\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.33.2\n",
            "    Uninstalling pydantic_core-2.33.2:\n",
            "      Successfully uninstalled pydantic_core-2.33.2\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.11.7\n",
            "    Uninstalling pydantic-2.11.7:\n",
            "      Successfully uninstalled pydantic-2.11.7\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.2\n",
            "    Uninstalling tokenizers-0.21.2:\n",
            "      Successfully uninstalled tokenizers-0.21.2\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.53.1\n",
            "    Uninstalling transformers-4.53.1:\n",
            "      Successfully uninstalled transformers-4.53.1\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.14.4\n",
            "    Uninstalling datasets-2.14.4:\n",
            "      Successfully uninstalled datasets-2.14.4\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 1.8.1\n",
            "    Uninstalling accelerate-1.8.1:\n",
            "      Successfully uninstalled accelerate-1.8.1\n",
            "  Attempting uninstall: peft\n",
            "    Found existing installation: peft 0.16.0\n",
            "    Uninstalling peft-0.16.0:\n",
            "      Successfully uninstalled peft-0.16.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-1.7.0 av-15.0.0 bitsandbytes-0.46.1 datasets-3.6.0 fire-0.7.0 fsspec-2025.3.0 llamafactory-0.9.4.dev0 modelscope-1.28.0 numpy-1.26.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 peft-0.15.2 pydantic-2.10.6 pydantic-core-2.27.2 shtab-1.7.2 sse-starlette-2.4.1 tokenizers-0.21.1 transformers-4.52.4 trl-0.9.6 tyro-0.8.14\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "05b2043225ac44aa83b9ee5a05b6c3e8"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Updating Identity Dataset"
      ],
      "metadata": {
        "id": "jDgULyME9Nfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "NAME = \"Llama-3\"\n",
        "AUTHOR = \"LLaMA Factory\"\n",
        "\n",
        "with open(\"data/identity.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "  dataset = json.load(f)\n",
        "\n",
        "for sample in dataset:\n",
        "  sample[\"output\"] = sample[\"output\"].replace(\"{{\"+ \"name\" + \"}}\", NAME).replace(\"{{\"+ \"author\" + \"}}\", AUTHOR)\n",
        "\n",
        "with open(\"data/identity.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "  json.dump(dataset, f, indent=2, ensure_ascii=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYA6NJlf8mIr",
        "outputId": "23ee1d72-ff36-40ee-9fd5-2352d1de412a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning via Command Line"
      ],
      "metadata": {
        "id": "ABBxsXAL9fbF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LoRA plus"
      ],
      "metadata": {
        "id": "26eDS7kbJrad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "args = dict(\n",
        "  stage=\"sft\",                                               # do supervised fine-tuning\n",
        "  do_train=True,\n",
        "  # model_name_or_path=\"unsloth/llama-3-8b-Instruct-bnb-4bit\", # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
        "  model_name_or_path=\"unsloth/llama-3.2-1b\",                   # use bnb-4bit-quantized Llama-3.2-1B model\n",
        "  dataset=\"identity,alpaca_en_demo\",                         # use alpaca and identity datasets\n",
        "  template=\"llama3\",                                         # use llama3 prompt template\n",
        "  finetuning_type=\"lora\",                                    # use LoRA adapters to save memory\n",
        "  lora_target=\"all\",                                         # attach LoRA adapters to all linear layers\n",
        "  output_dir=\"llama3.2_lora\",                                  # the path to save LoRA adapters\n",
        "  per_device_train_batch_size=2,                             # the micro batch size\n",
        "  gradient_accumulation_steps=4,                             # the gradient accumulation steps\n",
        "  lr_scheduler_type=\"cosine\",                                # use cosine learning rate scheduler\n",
        "  logging_steps=5,                                           # log every 5 steps\n",
        "  warmup_ratio=0.1,                                          # use warmup scheduler\n",
        "  save_steps=1000,                                           # save checkpoint every 1000 steps\n",
        "  learning_rate=5e-5,                                        # the learning rate\n",
        "  num_train_epochs=3.0,                                      # the epochs of training\n",
        "  max_samples=500,                                           # use 500 examples in each dataset\n",
        "  max_grad_norm=1.0,                                         # clip gradient norm to 1.0\n",
        "  loraplus_lr_ratio=16.0,                                    # use LoRA+ algorithm with lambda=16.0\n",
        "  fp16=True,                                                 # use float16 mixed precision training\n",
        "  report_to=\"wandb\",                                         # enable wandb logging\n",
        "  run_name=\"llama_factory_run1\"\n",
        ")\n",
        "\n",
        "json.dump(args, open(\"train_llama3.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "!llamafactory-cli train train_llama3.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "hFNrPYws9Q-5",
        "outputId": "4f5acdd3-da27-4c97-bd2e-56b604f3b68a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n",
            "2025-07-10 06:30:04.721685: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1752129004.742536    6707 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1752129004.748691    6707 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-07-10 06:30:04.770827: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[INFO|2025-07-10 06:30:12] llamafactory.hparams.parser:410 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: torch.float16\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-07-10 06:30:13,180 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3.2-1b/snapshots/9535bd9b1d1dea6acafbdc4813b728796aeb28da/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-07-10 06:30:13,180 >> loading file tokenizer.model from cache at None\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-07-10 06:30:13,180 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-07-10 06:30:13,180 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3.2-1b/snapshots/9535bd9b1d1dea6acafbdc4813b728796aeb28da/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-07-10 06:30:13,180 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3.2-1b/snapshots/9535bd9b1d1dea6acafbdc4813b728796aeb28da/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-07-10 06:30:13,180 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|tokenization_utils_base.py:2299] 2025-07-10 06:30:13,819 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|configuration_utils.py:698] 2025-07-10 06:30:14,969 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3.2-1b/snapshots/9535bd9b1d1dea6acafbdc4813b728796aeb28da/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-07-10 06:30:14,972 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"head_dim\": 64,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 16,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 128004,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 32.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"unsloth_fixed\": true,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-07-10 06:30:15,332 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3.2-1b/snapshots/9535bd9b1d1dea6acafbdc4813b728796aeb28da/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-07-10 06:30:15,332 >> loading file tokenizer.model from cache at None\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-07-10 06:30:15,333 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-07-10 06:30:15,333 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3.2-1b/snapshots/9535bd9b1d1dea6acafbdc4813b728796aeb28da/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-07-10 06:30:15,333 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3.2-1b/snapshots/9535bd9b1d1dea6acafbdc4813b728796aeb28da/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-07-10 06:30:15,333 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|tokenization_utils_base.py:2299] 2025-07-10 06:30:16,018 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|2025-07-10 06:30:16] llamafactory.data.template:143 >> Replace eos token: <|eot_id|>.\n",
            "[INFO|2025-07-10 06:30:16] llamafactory.data.template:143 >> Add <|eom_id|> to stop words.\n",
            "[INFO|2025-07-10 06:30:16] llamafactory.data.loader:143 >> Loading dataset identity.json...\n",
            "[INFO|2025-07-10 06:30:16] llamafactory.data.loader:143 >> Loading dataset alpaca_en_demo.json...\n",
            "training example:\n",
            "input_ids:\n",
            "[128000, 128006, 882, 128007, 271, 6151, 128009, 128006, 78191, 128007, 271, 9906, 0, 358, 1097, 445, 81101, 12, 18, 11, 459, 15592, 18328, 8040, 555, 445, 8921, 4940, 17367, 13, 2650, 649, 358, 7945, 499, 3432, 30, 128009]\n",
            "inputs:\n",
            "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "hi<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "Hello! I am Llama-3, an AI assistant developed by LLaMA Factory. How can I assist you today?<|eot_id|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 9906, 0, 358, 1097, 445, 81101, 12, 18, 11, 459, 15592, 18328, 8040, 555, 445, 8921, 4940, 17367, 13, 2650, 649, 358, 7945, 499, 3432, 30, 128009]\n",
            "labels:\n",
            "Hello! I am Llama-3, an AI assistant developed by LLaMA Factory. How can I assist you today?<|eot_id|>\n",
            "[INFO|configuration_utils.py:698] 2025-07-10 06:30:16,780 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3.2-1b/snapshots/9535bd9b1d1dea6acafbdc4813b728796aeb28da/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-07-10 06:30:16,781 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"head_dim\": 64,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 16,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 128004,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 32.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"unsloth_fixed\": true,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|2025-07-10 06:30:16] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.\n",
            "[INFO|modeling_utils.py:1151] 2025-07-10 06:30:17,063 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3.2-1b/snapshots/9535bd9b1d1dea6acafbdc4813b728796aeb28da/model.safetensors\n",
            "[INFO|modeling_utils.py:2241] 2025-07-10 06:30:17,063 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:1135] 2025-07-10 06:30:17,065 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"pad_token_id\": 128004,\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:5131] 2025-07-10 06:30:18,053 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:5139] 2025-07-10 06:30:18,053 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at unsloth/llama-3.2-1b.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:1090] 2025-07-10 06:30:18,237 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3.2-1b/snapshots/9535bd9b1d1dea6acafbdc4813b728796aeb28da/generation_config.json\n",
            "[INFO|configuration_utils.py:1135] 2025-07-10 06:30:18,238 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"max_length\": 131072,\n",
            "  \"pad_token_id\": 128004,\n",
            "  \"temperature\": 0.6,\n",
            "  \"top_p\": 0.9\n",
            "}\n",
            "\n",
            "[INFO|2025-07-10 06:30:18] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.\n",
            "[INFO|2025-07-10 06:30:18] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2025-07-10 06:30:18] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.\n",
            "[INFO|2025-07-10 06:30:18] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA\n",
            "[INFO|2025-07-10 06:30:18] llamafactory.model.model_utils.misc:143 >> Found linear modules: q_proj,gate_proj,up_proj,k_proj,down_proj,o_proj,v_proj\n",
            "[INFO|2025-07-10 06:30:18] llamafactory.model.loader:143 >> trainable params: 5,636,096 || all params: 1,241,450,496 || trainable%: 0.4540\n",
            "[INFO|trainer.py:756] 2025-07-10 06:30:18,884 >> Using auto half precision backend\n",
            "[INFO|2025-07-10 06:30:19] llamafactory.train.trainer_utils:143 >> Using LoRA+ optimizer with loraplus lr ratio 16.00.\n",
            "[INFO|trainer.py:2409] 2025-07-10 06:30:19,426 >> ***** Running training *****\n",
            "[INFO|trainer.py:2410] 2025-07-10 06:30:19,426 >>   Num examples = 591\n",
            "[INFO|trainer.py:2411] 2025-07-10 06:30:19,426 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2412] 2025-07-10 06:30:19,426 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:2415] 2025-07-10 06:30:19,426 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:2416] 2025-07-10 06:30:19,426 >>   Gradient Accumulation steps = 4\n",
            "[INFO|trainer.py:2417] 2025-07-10 06:30:19,426 >>   Total optimization steps = 222\n",
            "[INFO|trainer.py:2418] 2025-07-10 06:30:19,429 >>   Number of trainable parameters = 5,636,096\n",
            "[INFO|integration_utils.py:832] 2025-07-10 06:30:19,433 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madichats-2003\u001b[0m (\u001b[33madichats-2003-bits-pilani\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.21.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/LLaMA-Factory/wandb/run-20250710_063019-5a3nrf8c\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mllama_factory_run1\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/adichats-2003-bits-pilani/llamafactory\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/adichats-2003-bits-pilani/llamafactory/runs/5a3nrf8c\u001b[0m\n",
            "{'loss': 1.5723, 'grad_norm': 0.7389477491378784, 'learning_rate': 8.695652173913044e-06, 'epoch': 0.07}\n",
            "{'loss': 1.5808, 'grad_norm': 0.6671311855316162, 'learning_rate': 1.956521739130435e-05, 'epoch': 0.14}\n",
            "{'loss': 1.5614, 'grad_norm': 1.0436519384384155, 'learning_rate': 3.0434782608695656e-05, 'epoch': 0.2}\n",
            "{'loss': 1.6709, 'grad_norm': 1.0165821313858032, 'learning_rate': 4.130434782608696e-05, 'epoch': 0.27}\n",
            "{'loss': 1.361, 'grad_norm': 0.9819087982177734, 'learning_rate': 4.999688473794144e-05, 'epoch': 0.34}\n",
            "{'loss': 1.4147, 'grad_norm': 1.046599268913269, 'learning_rate': 4.9887932065027656e-05, 'epoch': 0.41}\n",
            "{'loss': 1.4775, 'grad_norm': 0.7616737484931946, 'learning_rate': 4.962399180850277e-05, 'epoch': 0.47}\n",
            "{'loss': 1.3788, 'grad_norm': 0.7241889238357544, 'learning_rate': 4.920670763496268e-05, 'epoch': 0.54}\n",
            "{'loss': 1.2989, 'grad_norm': 0.7893183827400208, 'learning_rate': 4.863867814784168e-05, 'epoch': 0.61}\n",
            "{'loss': 1.3456, 'grad_norm': 1.0162161588668823, 'learning_rate': 4.792344070481972e-05, 'epoch': 0.68}\n",
            "{'loss': 1.3274, 'grad_norm': 0.7845122814178467, 'learning_rate': 4.706544938921368e-05, 'epoch': 0.74}\n",
            "{'loss': 1.3591, 'grad_norm': 0.7885112166404724, 'learning_rate': 4.6070047272533765e-05, 'epoch': 0.81}\n",
            "{'loss': 1.4281, 'grad_norm': 0.688309907913208, 'learning_rate': 4.4943433140937986e-05, 'epoch': 0.88}\n",
            "{'loss': 1.3552, 'grad_norm': 1.3821909427642822, 'learning_rate': 4.369262289279273e-05, 'epoch': 0.95}\n",
            "{'loss': 1.2785, 'grad_norm': 0.8897034525871277, 'learning_rate': 4.2325405847733294e-05, 'epoch': 1.01}\n",
            "{'loss': 1.1617, 'grad_norm': 0.8316062688827515, 'learning_rate': 4.085029623930597e-05, 'epoch': 1.08}\n",
            "{'loss': 0.9557, 'grad_norm': 1.2673234939575195, 'learning_rate': 3.927648019326737e-05, 'epoch': 1.15}\n",
            "{'loss': 1.044, 'grad_norm': 0.7798254489898682, 'learning_rate': 3.7613758521729436e-05, 'epoch': 1.22}\n",
            "{'loss': 0.9746, 'grad_norm': 0.7437325119972229, 'learning_rate': 3.587248568939483e-05, 'epoch': 1.28}\n",
            "{'loss': 1.1385, 'grad_norm': 0.8909925222396851, 'learning_rate': 3.406350533196562e-05, 'epoch': 1.35}\n",
            "{'loss': 1.1308, 'grad_norm': 1.0145314931869507, 'learning_rate': 3.219808272827917e-05, 'epoch': 1.42}\n",
            "{'loss': 1.0544, 'grad_norm': 0.9522389769554138, 'learning_rate': 3.0287834646695477e-05, 'epoch': 1.49}\n",
            "{'loss': 0.9054, 'grad_norm': 0.8755038976669312, 'learning_rate': 2.834465700261198e-05, 'epoch': 1.55}\n",
            "{'loss': 1.0009, 'grad_norm': 1.2810149192810059, 'learning_rate': 2.6380650777612705e-05, 'epoch': 1.62}\n",
            "{'loss': 1.0518, 'grad_norm': 0.86308753490448, 'learning_rate': 2.4408046661584408e-05, 'epoch': 1.69}\n",
            "{'loss': 1.0568, 'grad_norm': 1.2115511894226074, 'learning_rate': 2.2439128887084673e-05, 'epoch': 1.76}\n",
            "{'loss': 1.1181, 'grad_norm': 1.405893325805664, 'learning_rate': 2.0486158730277454e-05, 'epoch': 1.82}\n",
            "{'loss': 1.063, 'grad_norm': 0.7390722036361694, 'learning_rate': 1.856129815482759e-05, 'epoch': 1.89}\n",
            "{'loss': 1.0456, 'grad_norm': 2.217397689819336, 'learning_rate': 1.667653407425598e-05, 'epoch': 1.96}\n",
            "{'loss': 0.9075, 'grad_norm': 1.1468689441680908, 'learning_rate': 1.4843603704405279e-05, 'epoch': 2.03}\n",
            "{'loss': 0.7939, 'grad_norm': 0.8844977021217346, 'learning_rate': 1.307392147087777e-05, 'epoch': 2.09}\n",
            "{'loss': 0.7756, 'grad_norm': 1.2488911151885986, 'learning_rate': 1.1378507926623247e-05, 'epoch': 2.16}\n",
            "{'loss': 0.7545, 'grad_norm': 0.8977794647216797, 'learning_rate': 9.76792112233709e-06, 'epoch': 2.23}\n",
            "{'loss': 0.8366, 'grad_norm': 0.9234464764595032, 'learning_rate': 8.252190857053626e-06, 'epoch': 2.3}\n",
            "{'loss': 0.8219, 'grad_norm': 0.8552572131156921, 'learning_rate': 6.840756218384023e-06, 'epoch': 2.36}\n",
            "{'loss': 0.7876, 'grad_norm': 1.2238446474075317, 'learning_rate': 5.542406801361758e-06, 'epoch': 2.43}\n",
            "{'loss': 0.8253, 'grad_norm': 0.8175197839736938, 'learning_rate': 4.3652279719506e-06, 'epoch': 2.5}\n",
            "{'loss': 0.9007, 'grad_norm': 1.2479192018508911, 'learning_rate': 3.316550516082137e-06, 'epoch': 2.57}\n",
            "{'loss': 0.7764, 'grad_norm': 0.9458710551261902, 'learning_rate': 2.402904987779414e-06, 'epoch': 2.64}\n",
            "{'loss': 0.6085, 'grad_norm': 0.9249597787857056, 'learning_rate': 1.6299810406600419e-06, 'epoch': 2.7}\n",
            "{'loss': 0.7507, 'grad_norm': 1.349409580230713, 'learning_rate': 1.0025919960785724e-06, 'epoch': 2.77}\n",
            "{'loss': 0.9002, 'grad_norm': 1.2338824272155762, 'learning_rate': 5.246448685571365e-07, 'epoch': 2.84}\n",
            "{'loss': 0.8348, 'grad_norm': 0.8618574738502502, 'learning_rate': 1.9911603516855338e-07, 'epoch': 2.91}\n",
            "{'loss': 0.8064, 'grad_norm': 0.7999216318130493, 'learning_rate': 2.8032700388910814e-08, 'epoch': 2.97}\n",
            "100% 222/222 [04:24<00:00,  1.05s/it][INFO|trainer.py:3993] 2025-07-10 06:34:44,831 >> Saving model checkpoint to llama3.2_lora/checkpoint-222\n",
            "[INFO|configuration_utils.py:698] 2025-07-10 06:34:45,226 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3.2-1b/snapshots/9535bd9b1d1dea6acafbdc4813b728796aeb28da/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-07-10 06:34:45,227 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"head_dim\": 64,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 16,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 128004,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 32.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"unsloth_fixed\": true,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2356] 2025-07-10 06:34:45,302 >> chat template saved in llama3.2_lora/checkpoint-222/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2525] 2025-07-10 06:34:45,308 >> tokenizer config file saved in llama3.2_lora/checkpoint-222/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2534] 2025-07-10 06:34:45,308 >> Special tokens file saved in llama3.2_lora/checkpoint-222/special_tokens_map.json\n",
            "[INFO|trainer.py:2676] 2025-07-10 06:34:45,790 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 266.3613, 'train_samples_per_second': 6.656, 'train_steps_per_second': 0.833, 'train_loss': 1.0919294899648375, 'epoch': 3.0}\n",
            "100% 222/222 [04:25<00:00,  1.19s/it]\n",
            "[INFO|trainer.py:3993] 2025-07-10 06:34:45,796 >> Saving model checkpoint to llama3.2_lora\n",
            "[INFO|configuration_utils.py:698] 2025-07-10 06:34:46,211 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3.2-1b/snapshots/9535bd9b1d1dea6acafbdc4813b728796aeb28da/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-07-10 06:34:46,213 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"head_dim\": 64,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 16,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 128004,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 32.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"unsloth_fixed\": true,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2356] 2025-07-10 06:34:46,279 >> chat template saved in llama3.2_lora/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2525] 2025-07-10 06:34:46,285 >> tokenizer config file saved in llama3.2_lora/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2534] 2025-07-10 06:34:46,285 >> Special tokens file saved in llama3.2_lora/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  total_flos               =  2109864GF\n",
            "  train_loss               =     1.0919\n",
            "  train_runtime            = 0:04:26.36\n",
            "  train_samples_per_second =      6.656\n",
            "  train_steps_per_second   =      0.833\n",
            "[INFO|modelcard.py:450] 2025-07-10 06:34:46,668 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mllama_factory_run1\u001b[0m at: \u001b[34mhttps://wandb.ai/adichats-2003-bits-pilani/llamafactory/runs/5a3nrf8c\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250710_063019-5a3nrf8c/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Freeze Training"
      ],
      "metadata": {
        "id": "ehsNaYnYJxAG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "args = dict(\n",
        "  stage=\"sft\",                                               # do supervised fine-tuning\n",
        "  do_train= True,\n",
        "  model_name_or_path=\"unsloth/llama-3.2-1b\",                   # use bnb-4bit-quantized Llama-3.2-1B model\n",
        "  finetuning_type= \"freeze\",\n",
        "  freeze_trainable_layers= 2,\n",
        "  freeze_trainable_modules= \"all\",\n",
        "  dataset=\"identity,alpaca_en_demo\",                         # use alpaca and identity datasets\n",
        "  template=\"llama3\",                                         # use llama3 prompt template\n",
        "  output_dir= \"outputs/freeze\",\n",
        "  per_device_train_batch_size= 2,\n",
        "  gradient_accumulation_steps=4,                             # the gradient accumulation steps\n",
        "  num_train_epochs= 3,\n",
        "  lr_scheduler_type=\"cosine\",                                # use cosine learning rate scheduler\n",
        "  learning_rate=5e-5,                                        # the learning rate\n",
        "  warmup_ratio=0.1,                                          # use warmup scheduler\n",
        "  fp16= True,\n",
        "  logging_steps= 5,\n",
        "  save_steps= 1000,\n",
        "  max_samples=500,                                           # use 500 examples in each dataset\n",
        "  max_grad_norm=1.0,                                         # clip gradient norm to 1.0\n",
        "  plot_loss= True,\n",
        "  overwrite_cache= True,\n",
        "  report_to= \"wandb\",\n",
        "  run_name=\"llama_factory_run2_freeze\"\n",
        ")\n",
        "\n",
        "json.dump(args, open(\"train_llama3_freeze.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "!llamafactory-cli train train_llama3_freeze.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "A2X0eu1O-Ivw",
        "outputId": "fcdfda19-b531-4453-b3cf-5f47e532c18e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n",
            "2025-07-10 06:53:50.730187: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1752130430.751585   12720 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1752130430.757907   12720 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-07-10 06:53:50.779305: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[INFO|2025-07-10 06:53:58] llamafactory.hparams.parser:410 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: torch.float16\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-07-10 06:53:59,226 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3.2-1b/snapshots/9535bd9b1d1dea6acafbdc4813b728796aeb28da/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-07-10 06:53:59,226 >> loading file tokenizer.model from cache at None\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-07-10 06:53:59,226 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-07-10 06:53:59,226 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3.2-1b/snapshots/9535bd9b1d1dea6acafbdc4813b728796aeb28da/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-07-10 06:53:59,226 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3.2-1b/snapshots/9535bd9b1d1dea6acafbdc4813b728796aeb28da/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-07-10 06:53:59,226 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|tokenization_utils_base.py:2299] 2025-07-10 06:53:59,872 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|configuration_utils.py:698] 2025-07-10 06:54:00,919 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3.2-1b/snapshots/9535bd9b1d1dea6acafbdc4813b728796aeb28da/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-07-10 06:54:00,922 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"head_dim\": 64,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 16,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 128004,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 32.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"unsloth_fixed\": true,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-07-10 06:54:01,261 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3.2-1b/snapshots/9535bd9b1d1dea6acafbdc4813b728796aeb28da/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-07-10 06:54:01,261 >> loading file tokenizer.model from cache at None\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-07-10 06:54:01,261 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-07-10 06:54:01,261 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3.2-1b/snapshots/9535bd9b1d1dea6acafbdc4813b728796aeb28da/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-07-10 06:54:01,261 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3.2-1b/snapshots/9535bd9b1d1dea6acafbdc4813b728796aeb28da/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-07-10 06:54:01,261 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|tokenization_utils_base.py:2299] 2025-07-10 06:54:01,928 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|2025-07-10 06:54:01] llamafactory.data.template:143 >> Replace eos token: <|eot_id|>.\n",
            "[INFO|2025-07-10 06:54:01] llamafactory.data.template:143 >> Add <|eom_id|> to stop words.\n",
            "[INFO|2025-07-10 06:54:01] llamafactory.data.loader:143 >> Loading dataset identity.json...\n",
            "Converting format of dataset: 100% 91/91 [00:00<00:00, 7939.46 examples/s]\n",
            "[INFO|2025-07-10 06:54:02] llamafactory.data.loader:143 >> Loading dataset alpaca_en_demo.json...\n",
            "Converting format of dataset: 100% 500/500 [00:00<00:00, 14804.33 examples/s]\n",
            "Running tokenizer on dataset: 100% 591/591 [00:00<00:00, 1526.69 examples/s]\n",
            "training example:\n",
            "input_ids:\n",
            "[128000, 128006, 882, 128007, 271, 6151, 128009, 128006, 78191, 128007, 271, 9906, 0, 358, 1097, 445, 81101, 12, 18, 11, 459, 15592, 18328, 8040, 555, 445, 8921, 4940, 17367, 13, 2650, 649, 358, 7945, 499, 3432, 30, 128009]\n",
            "inputs:\n",
            "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "hi<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "Hello! I am Llama-3, an AI assistant developed by LLaMA Factory. How can I assist you today?<|eot_id|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 9906, 0, 358, 1097, 445, 81101, 12, 18, 11, 459, 15592, 18328, 8040, 555, 445, 8921, 4940, 17367, 13, 2650, 649, 358, 7945, 499, 3432, 30, 128009]\n",
            "labels:\n",
            "Hello! I am Llama-3, an AI assistant developed by LLaMA Factory. How can I assist you today?<|eot_id|>\n",
            "[INFO|configuration_utils.py:698] 2025-07-10 06:54:03,068 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3.2-1b/snapshots/9535bd9b1d1dea6acafbdc4813b728796aeb28da/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-07-10 06:54:03,069 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"head_dim\": 64,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 16,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 128004,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 32.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"unsloth_fixed\": true,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|2025-07-10 06:54:03] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.\n",
            "[INFO|modeling_utils.py:1151] 2025-07-10 06:54:03,334 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3.2-1b/snapshots/9535bd9b1d1dea6acafbdc4813b728796aeb28da/model.safetensors\n",
            "[INFO|modeling_utils.py:2241] 2025-07-10 06:54:03,334 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:1135] 2025-07-10 06:54:03,336 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"pad_token_id\": 128004,\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:5131] 2025-07-10 06:54:04,337 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:5139] 2025-07-10 06:54:04,337 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at unsloth/llama-3.2-1b.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:1090] 2025-07-10 06:54:04,525 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3.2-1b/snapshots/9535bd9b1d1dea6acafbdc4813b728796aeb28da/generation_config.json\n",
            "[INFO|configuration_utils.py:1135] 2025-07-10 06:54:04,525 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"max_length\": 131072,\n",
            "  \"pad_token_id\": 128004,\n",
            "  \"temperature\": 0.6,\n",
            "  \"top_p\": 0.9\n",
            "}\n",
            "\n",
            "[INFO|2025-07-10 06:54:04] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.\n",
            "[INFO|2025-07-10 06:54:04] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2025-07-10 06:54:04] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.\n",
            "[INFO|2025-07-10 06:54:04] llamafactory.model.adapter:143 >> Fine-tuning method: Freeze\n",
            "[INFO|2025-07-10 06:54:04] llamafactory.model.adapter:143 >> Set trainable layers: .14.,.15.\n",
            "[INFO|2025-07-10 06:54:04] llamafactory.model.loader:143 >> trainable params: 121,643,008 || all params: 1,235,814,400 || trainable%: 9.8431\n",
            "[INFO|trainer.py:756] 2025-07-10 06:54:04,719 >> Using auto half precision backend\n",
            "[INFO|trainer.py:2409] 2025-07-10 06:54:05,120 >> ***** Running training *****\n",
            "[INFO|trainer.py:2410] 2025-07-10 06:54:05,121 >>   Num examples = 591\n",
            "[INFO|trainer.py:2411] 2025-07-10 06:54:05,121 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2412] 2025-07-10 06:54:05,121 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:2415] 2025-07-10 06:54:05,121 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:2416] 2025-07-10 06:54:05,121 >>   Gradient Accumulation steps = 4\n",
            "[INFO|trainer.py:2417] 2025-07-10 06:54:05,121 >>   Total optimization steps = 222\n",
            "[INFO|trainer.py:2418] 2025-07-10 06:54:05,121 >>   Number of trainable parameters = 121,643,008\n",
            "[INFO|integration_utils.py:832] 2025-07-10 06:54:05,122 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madichats-2003\u001b[0m (\u001b[33madichats-2003-bits-pilani\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.21.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/LLaMA-Factory/wandb/run-20250710_065405-3qa3wqou\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mllama_factory_run2_freeze\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/adichats-2003-bits-pilani/llamafactory\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/adichats-2003-bits-pilani/llamafactory/runs/3qa3wqou\u001b[0m\n",
            "{'loss': 1.5772, 'grad_norm': 2.6840384006500244, 'learning_rate': 8.695652173913044e-06, 'epoch': 0.07}\n",
            "{'loss': 1.6275, 'grad_norm': 2.143855094909668, 'learning_rate': 1.956521739130435e-05, 'epoch': 0.14}\n",
            "{'loss': 1.6367, 'grad_norm': 3.385122776031494, 'learning_rate': 3.0434782608695656e-05, 'epoch': 0.2}\n",
            "{'loss': 1.7908, 'grad_norm': 4.0526123046875, 'learning_rate': 4.130434782608696e-05, 'epoch': 0.27}\n",
            "{'loss': 1.4443, 'grad_norm': 3.4999735355377197, 'learning_rate': 4.999688473794144e-05, 'epoch': 0.34}\n",
            "{'loss': 1.535, 'grad_norm': 2.7627506256103516, 'learning_rate': 4.9887932065027656e-05, 'epoch': 0.41}\n",
            "{'loss': 1.6217, 'grad_norm': 2.55688738822937, 'learning_rate': 4.962399180850277e-05, 'epoch': 0.47}\n",
            "{'loss': 1.4962, 'grad_norm': 2.8380537033081055, 'learning_rate': 4.920670763496268e-05, 'epoch': 0.54}\n",
            "{'loss': 1.4427, 'grad_norm': 3.4736456871032715, 'learning_rate': 4.863867814784168e-05, 'epoch': 0.61}\n",
            "{'loss': 1.4904, 'grad_norm': 4.614113807678223, 'learning_rate': 4.792344070481972e-05, 'epoch': 0.68}\n",
            "{'loss': 1.4657, 'grad_norm': 2.7129065990448, 'learning_rate': 4.706544938921368e-05, 'epoch': 0.74}\n",
            "{'loss': 1.5295, 'grad_norm': 2.9346578121185303, 'learning_rate': 4.6070047272533765e-05, 'epoch': 0.81}\n",
            "{'loss': 1.5763, 'grad_norm': 2.412421941757202, 'learning_rate': 4.4943433140937986e-05, 'epoch': 0.88}\n",
            "{'loss': 1.4994, 'grad_norm': 4.061525344848633, 'learning_rate': 4.369262289279273e-05, 'epoch': 0.95}\n",
            "{'loss': 1.3579, 'grad_norm': 2.941676616668701, 'learning_rate': 4.2325405847733294e-05, 'epoch': 1.01}\n",
            "{'loss': 1.1321, 'grad_norm': 2.960263252258301, 'learning_rate': 4.085029623930597e-05, 'epoch': 1.08}\n",
            "{'loss': 0.9513, 'grad_norm': 3.9661853313446045, 'learning_rate': 3.927648019326737e-05, 'epoch': 1.15}\n",
            "{'loss': 1.0504, 'grad_norm': 2.0973498821258545, 'learning_rate': 3.7613758521729436e-05, 'epoch': 1.22}\n",
            "{'loss': 0.9055, 'grad_norm': 1.9185757637023926, 'learning_rate': 3.587248568939483e-05, 'epoch': 1.28}\n",
            "{'loss': 1.0883, 'grad_norm': 2.827017068862915, 'learning_rate': 3.406350533196562e-05, 'epoch': 1.35}\n",
            "{'loss': 1.1058, 'grad_norm': 3.2336573600769043, 'learning_rate': 3.219808272827917e-05, 'epoch': 1.42}\n",
            "{'loss': 0.9708, 'grad_norm': 2.892075777053833, 'learning_rate': 3.0287834646695477e-05, 'epoch': 1.49}\n",
            "{'loss': 0.8558, 'grad_norm': 2.8681435585021973, 'learning_rate': 2.834465700261198e-05, 'epoch': 1.55}\n",
            "{'loss': 0.9471, 'grad_norm': 3.131682872772217, 'learning_rate': 2.6380650777612705e-05, 'epoch': 1.62}\n",
            "{'loss': 1.0105, 'grad_norm': 2.7853260040283203, 'learning_rate': 2.4408046661584408e-05, 'epoch': 1.69}\n",
            "{'loss': 0.9942, 'grad_norm': 3.0599145889282227, 'learning_rate': 2.2439128887084673e-05, 'epoch': 1.76}\n",
            "{'loss': 1.0794, 'grad_norm': 4.288682460784912, 'learning_rate': 2.0486158730277454e-05, 'epoch': 1.82}\n",
            "{'loss': 0.9889, 'grad_norm': 2.4862735271453857, 'learning_rate': 1.856129815482759e-05, 'epoch': 1.89}\n",
            "{'loss': 1.0386, 'grad_norm': 6.560484409332275, 'learning_rate': 1.667653407425598e-05, 'epoch': 1.96}\n",
            "{'loss': 0.8976, 'grad_norm': 2.6427783966064453, 'learning_rate': 1.4843603704405279e-05, 'epoch': 2.03}\n",
            "{'loss': 0.67, 'grad_norm': 2.0781912803649902, 'learning_rate': 1.307392147087777e-05, 'epoch': 2.09}\n",
            "{'loss': 0.6758, 'grad_norm': 2.5842418670654297, 'learning_rate': 1.1378507926623247e-05, 'epoch': 2.16}\n",
            "{'loss': 0.6607, 'grad_norm': 1.6929904222488403, 'learning_rate': 9.76792112233709e-06, 'epoch': 2.23}\n",
            "{'loss': 0.7063, 'grad_norm': 2.1788599491119385, 'learning_rate': 8.252190857053626e-06, 'epoch': 2.3}\n",
            "{'loss': 0.7173, 'grad_norm': 2.060565233230591, 'learning_rate': 6.840756218384023e-06, 'epoch': 2.36}\n",
            "{'loss': 0.6572, 'grad_norm': 2.156491756439209, 'learning_rate': 5.542406801361758e-06, 'epoch': 2.43}\n",
            "{'loss': 0.7269, 'grad_norm': 1.9312617778778076, 'learning_rate': 4.3652279719506e-06, 'epoch': 2.5}\n",
            "{'loss': 0.7636, 'grad_norm': 2.590975046157837, 'learning_rate': 3.316550516082137e-06, 'epoch': 2.57}\n",
            "{'loss': 0.6755, 'grad_norm': 2.1978354454040527, 'learning_rate': 2.402904987779414e-06, 'epoch': 2.64}\n",
            "{'loss': 0.5706, 'grad_norm': 1.8887814283370972, 'learning_rate': 1.6299810406600419e-06, 'epoch': 2.7}\n",
            "{'loss': 0.6872, 'grad_norm': 3.824051856994629, 'learning_rate': 1.0025919960785724e-06, 'epoch': 2.77}\n",
            "{'loss': 0.7433, 'grad_norm': 3.0468997955322266, 'learning_rate': 5.246448685571365e-07, 'epoch': 2.84}\n",
            "{'loss': 0.698, 'grad_norm': 1.9770560264587402, 'learning_rate': 1.9911603516855338e-07, 'epoch': 2.91}\n",
            "{'loss': 0.699, 'grad_norm': 1.962211012840271, 'learning_rate': 2.8032700388910814e-08, 'epoch': 2.97}\n",
            "100% 222/222 [01:57<00:00,  2.11it/s][INFO|trainer.py:3993] 2025-07-10 06:56:03,437 >> Saving model checkpoint to outputs/freeze/checkpoint-222\n",
            "[INFO|configuration_utils.py:424] 2025-07-10 06:56:03,440 >> Configuration saved in outputs/freeze/checkpoint-222/config.json\n",
            "[INFO|configuration_utils.py:904] 2025-07-10 06:56:03,441 >> Configuration saved in outputs/freeze/checkpoint-222/generation_config.json\n",
            "[INFO|modeling_utils.py:3725] 2025-07-10 06:56:46,576 >> Model weights saved in outputs/freeze/checkpoint-222/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2356] 2025-07-10 06:56:46,577 >> chat template saved in outputs/freeze/checkpoint-222/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2525] 2025-07-10 06:56:46,580 >> tokenizer config file saved in outputs/freeze/checkpoint-222/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2534] 2025-07-10 06:56:46,581 >> Special tokens file saved in outputs/freeze/checkpoint-222/special_tokens_map.json\n",
            "[INFO|trainer.py:2676] 2025-07-10 06:56:53,375 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 168.2545, 'train_samples_per_second': 10.538, 'train_steps_per_second': 1.319, 'train_loss': 1.0817508874712765, 'epoch': 3.0}\n",
            "100% 222/222 [02:47<00:00,  1.33it/s]\n",
            "[INFO|trainer.py:3993] 2025-07-10 06:56:53,384 >> Saving model checkpoint to outputs/freeze\n",
            "[INFO|configuration_utils.py:424] 2025-07-10 06:56:53,388 >> Configuration saved in outputs/freeze/config.json\n",
            "[INFO|configuration_utils.py:904] 2025-07-10 06:56:53,389 >> Configuration saved in outputs/freeze/generation_config.json\n",
            "[INFO|modeling_utils.py:3725] 2025-07-10 06:57:39,435 >> Model weights saved in outputs/freeze/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2356] 2025-07-10 06:57:39,438 >> chat template saved in outputs/freeze/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2525] 2025-07-10 06:57:39,444 >> tokenizer config file saved in outputs/freeze/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2534] 2025-07-10 06:57:39,445 >> Special tokens file saved in outputs/freeze/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  total_flos               =  2097715GF\n",
            "  train_loss               =     1.0818\n",
            "  train_runtime            = 0:02:48.25\n",
            "  train_samples_per_second =     10.538\n",
            "  train_steps_per_second   =      1.319\n",
            "Figure saved at: outputs/freeze/training_loss.png\n",
            "[WARNING|2025-07-10 06:57:40] llamafactory.extras.ploting:148 >> No metric eval_loss to plot.\n",
            "[WARNING|2025-07-10 06:57:40] llamafactory.extras.ploting:148 >> No metric eval_accuracy to plot.\n",
            "[INFO|modelcard.py:450] 2025-07-10 06:57:40,280 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mllama_factory_run2_freeze\u001b[0m at: \u001b[34mhttps://wandb.ai/adichats-2003-bits-pilani/llamafactory/runs/3qa3wqou\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250710_065405-3qa3wqou/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Base Arguments"
      ],
      "metadata": {
        "id": "ytMFAeKKOIxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_args = dict(\n",
        "  stage=\"sft\",\n",
        "  do_train=True,\n",
        "  model_name_or_path=\"unsloth/llama-3.2-1b\",\n",
        "  dataset=\"identity,alpaca_en_demo\",\n",
        "  template=\"llama3\",\n",
        "  # output_dir=\"\",  # ← Will be different per method\n",
        "  per_device_train_batch_size=2,\n",
        "  gradient_accumulation_steps=4,\n",
        "  lr_scheduler_type=\"cosine\",\n",
        "  logging_steps=5,\n",
        "  warmup_ratio=0.1,\n",
        "  save_steps=1000,\n",
        "  learning_rate=5e-5,\n",
        "  num_train_epochs=3.0,\n",
        "  max_samples=500,\n",
        "  max_grad_norm=1.0,\n",
        "  fp16=True,\n",
        "  report_to=\"wandb\",\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "x7vV_YuHDYz-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DoRA"
      ],
      "metadata": {
        "id": "wa0b_h1cOLdF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dora_args = dict(\n",
        "  **base_args,\n",
        "  finetuning_type=\"lora\",\n",
        "  lora_target=\"all\",\n",
        "  use_dora=True,\n",
        "  output_dir=\"llama3.2_dora\",\n",
        "  run_name=\"llama_factory_run3_dora\"\n",
        ")\n",
        "\n",
        "json.dump(dora_args, open(\"train_llama3_DoRA.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "!llamafactory-cli train train_llama3_DoRA.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rj_Q76r1JNwE",
        "outputId": "c294fab8-cead-45a3-cc9b-6ac8af50f536"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n",
            "2025-07-10 07:09:18.152772: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1752131358.172612   16684 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1752131358.178610   16684 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-07-10 07:09:18.198834: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[INFO|2025-07-10 07:09:26] llamafactory.hparams.parser:410 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: torch.float16\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-07-10 07:09:27,275 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3.2-1b/snapshots/9535bd9b1d1dea6acafbdc4813b728796aeb28da/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-07-10 07:09:27,276 >> loading file tokenizer.model from cache at None\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-07-10 07:09:27,276 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-07-10 07:09:27,276 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3.2-1b/snapshots/9535bd9b1d1dea6acafbdc4813b728796aeb28da/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-07-10 07:09:27,276 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3.2-1b/snapshots/9535bd9b1d1dea6acafbdc4813b728796aeb28da/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-07-10 07:09:27,276 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|tokenization_utils_base.py:2299] 2025-07-10 07:09:27,917 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|configuration_utils.py:698] 2025-07-10 07:09:28,961 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3.2-1b/snapshots/9535bd9b1d1dea6acafbdc4813b728796aeb28da/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-07-10 07:09:28,964 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"head_dim\": 64,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 16,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 128004,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 32.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"unsloth_fixed\": true,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-07-10 07:09:29,342 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3.2-1b/snapshots/9535bd9b1d1dea6acafbdc4813b728796aeb28da/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-07-10 07:09:29,342 >> loading file tokenizer.model from cache at None\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-07-10 07:09:29,342 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-07-10 07:09:29,342 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3.2-1b/snapshots/9535bd9b1d1dea6acafbdc4813b728796aeb28da/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-07-10 07:09:29,342 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3.2-1b/snapshots/9535bd9b1d1dea6acafbdc4813b728796aeb28da/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-07-10 07:09:29,343 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|tokenization_utils_base.py:2299] 2025-07-10 07:09:30,021 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|2025-07-10 07:09:30] llamafactory.data.template:143 >> Replace eos token: <|eot_id|>.\n",
            "[INFO|2025-07-10 07:09:30] llamafactory.data.template:143 >> Add <|eom_id|> to stop words.\n",
            "[INFO|2025-07-10 07:09:30] llamafactory.data.loader:143 >> Loading dataset identity.json...\n",
            "[INFO|2025-07-10 07:09:30] llamafactory.data.loader:143 >> Loading dataset alpaca_en_demo.json...\n",
            "training example:\n",
            "input_ids:\n",
            "[128000, 128006, 882, 128007, 271, 6151, 128009, 128006, 78191, 128007, 271, 9906, 0, 358, 1097, 445, 81101, 12, 18, 11, 459, 15592, 18328, 8040, 555, 445, 8921, 4940, 17367, 13, 2650, 649, 358, 7945, 499, 3432, 30, 128009]\n",
            "inputs:\n",
            "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "hi<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "Hello! I am Llama-3, an AI assistant developed by LLaMA Factory. How can I assist you today?<|eot_id|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 9906, 0, 358, 1097, 445, 81101, 12, 18, 11, 459, 15592, 18328, 8040, 555, 445, 8921, 4940, 17367, 13, 2650, 649, 358, 7945, 499, 3432, 30, 128009]\n",
            "labels:\n",
            "Hello! I am Llama-3, an AI assistant developed by LLaMA Factory. How can I assist you today?<|eot_id|>\n",
            "[INFO|configuration_utils.py:698] 2025-07-10 07:09:30,765 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3.2-1b/snapshots/9535bd9b1d1dea6acafbdc4813b728796aeb28da/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-07-10 07:09:30,766 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"head_dim\": 64,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 16,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 128004,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 32.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"unsloth_fixed\": true,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|2025-07-10 07:09:30] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.\n",
            "[INFO|modeling_utils.py:1151] 2025-07-10 07:09:31,036 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3.2-1b/snapshots/9535bd9b1d1dea6acafbdc4813b728796aeb28da/model.safetensors\n",
            "[INFO|modeling_utils.py:2241] 2025-07-10 07:09:31,037 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:1135] 2025-07-10 07:09:31,039 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"pad_token_id\": 128004,\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:5131] 2025-07-10 07:09:32,145 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:5139] 2025-07-10 07:09:32,145 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at unsloth/llama-3.2-1b.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:1090] 2025-07-10 07:09:32,340 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3.2-1b/snapshots/9535bd9b1d1dea6acafbdc4813b728796aeb28da/generation_config.json\n",
            "[INFO|configuration_utils.py:1135] 2025-07-10 07:09:32,340 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"max_length\": 131072,\n",
            "  \"pad_token_id\": 128004,\n",
            "  \"temperature\": 0.6,\n",
            "  \"top_p\": 0.9\n",
            "}\n",
            "\n",
            "[INFO|2025-07-10 07:09:32] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.\n",
            "[INFO|2025-07-10 07:09:32] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2025-07-10 07:09:32] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.\n",
            "[INFO|2025-07-10 07:09:32] llamafactory.model.adapter:143 >> Fine-tuning method: DoRA\n",
            "[INFO|2025-07-10 07:09:32] llamafactory.model.model_utils.misc:143 >> Found linear modules: o_proj,up_proj,down_proj,v_proj,gate_proj,q_proj,k_proj\n",
            "[INFO|2025-07-10 07:09:33] llamafactory.model.loader:143 >> trainable params: 6,012,928 || all params: 1,241,827,328 || trainable%: 0.4842\n",
            "[INFO|trainer.py:756] 2025-07-10 07:09:33,233 >> Using auto half precision backend\n",
            "[INFO|trainer.py:2409] 2025-07-10 07:09:33,677 >> ***** Running training *****\n",
            "[INFO|trainer.py:2410] 2025-07-10 07:09:33,677 >>   Num examples = 591\n",
            "[INFO|trainer.py:2411] 2025-07-10 07:09:33,677 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2412] 2025-07-10 07:09:33,677 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:2415] 2025-07-10 07:09:33,677 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:2416] 2025-07-10 07:09:33,677 >>   Gradient Accumulation steps = 4\n",
            "[INFO|trainer.py:2417] 2025-07-10 07:09:33,677 >>   Total optimization steps = 222\n",
            "[INFO|trainer.py:2418] 2025-07-10 07:09:33,679 >>   Number of trainable parameters = 6,012,928\n",
            "[INFO|integration_utils.py:832] 2025-07-10 07:09:33,682 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madichats-2003\u001b[0m (\u001b[33madichats-2003-bits-pilani\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.21.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/LLaMA-Factory/wandb/run-20250710_070933-qybr1rwy\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mllama_factory_run3_dora\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/adichats-2003-bits-pilani/llamafactory\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/adichats-2003-bits-pilani/llamafactory/runs/qybr1rwy\u001b[0m\n",
            "{'loss': 1.5786, 'grad_norm': 0.9410117864608765, 'learning_rate': 8.695652173913044e-06, 'epoch': 0.07}\n",
            "{'loss': 1.6579, 'grad_norm': 0.7449998259544373, 'learning_rate': 1.956521739130435e-05, 'epoch': 0.14}\n",
            "{'loss': 1.69, 'grad_norm': 1.2077603340148926, 'learning_rate': 3.0434782608695656e-05, 'epoch': 0.2}\n",
            "{'loss': 1.8832, 'grad_norm': 1.6868606805801392, 'learning_rate': 4.130434782608696e-05, 'epoch': 0.27}\n",
            "{'loss': 1.507, 'grad_norm': 1.2353215217590332, 'learning_rate': 4.999688473794144e-05, 'epoch': 0.34}\n",
            "{'loss': 1.556, 'grad_norm': 1.0184857845306396, 'learning_rate': 4.9887932065027656e-05, 'epoch': 0.41}\n",
            "{'loss': 1.5779, 'grad_norm': 0.8770039081573486, 'learning_rate': 4.962399180850277e-05, 'epoch': 0.47}\n",
            "{'loss': 1.5067, 'grad_norm': 1.1335231065750122, 'learning_rate': 4.920670763496268e-05, 'epoch': 0.54}\n",
            "{'loss': 1.4552, 'grad_norm': 1.2001655101776123, 'learning_rate': 4.863867814784168e-05, 'epoch': 0.61}\n",
            "{'loss': 1.4007, 'grad_norm': 1.6266746520996094, 'learning_rate': 4.792344070481972e-05, 'epoch': 0.68}\n",
            "{'loss': 1.4173, 'grad_norm': 1.2695369720458984, 'learning_rate': 4.706544938921368e-05, 'epoch': 0.74}\n",
            "{'loss': 1.4152, 'grad_norm': 1.1426869630813599, 'learning_rate': 4.6070047272533765e-05, 'epoch': 0.81}\n",
            "{'loss': 1.4686, 'grad_norm': 1.175366759300232, 'learning_rate': 4.4943433140937986e-05, 'epoch': 0.88}\n",
            "{'loss': 1.3863, 'grad_norm': 2.5401110649108887, 'learning_rate': 4.369262289279273e-05, 'epoch': 0.95}\n",
            "{'loss': 1.3942, 'grad_norm': 1.5077553987503052, 'learning_rate': 4.2325405847733294e-05, 'epoch': 1.01}\n",
            "{'loss': 1.4506, 'grad_norm': 1.3330281972885132, 'learning_rate': 4.085029623930597e-05, 'epoch': 1.08}\n",
            "{'loss': 1.232, 'grad_norm': 2.95455002784729, 'learning_rate': 3.927648019326737e-05, 'epoch': 1.15}\n",
            "{'loss': 1.3619, 'grad_norm': 1.288624882698059, 'learning_rate': 3.7613758521729436e-05, 'epoch': 1.22}\n",
            "{'loss': 1.2687, 'grad_norm': 1.1309605836868286, 'learning_rate': 3.587248568939483e-05, 'epoch': 1.28}\n",
            "{'loss': 1.4045, 'grad_norm': 1.2850003242492676, 'learning_rate': 3.406350533196562e-05, 'epoch': 1.35}\n",
            "{'loss': 1.5055, 'grad_norm': 1.9372022151947021, 'learning_rate': 3.219808272827917e-05, 'epoch': 1.42}\n",
            "{'loss': 1.3686, 'grad_norm': 1.5001226663589478, 'learning_rate': 3.0287834646695477e-05, 'epoch': 1.49}\n",
            "{'loss': 1.1668, 'grad_norm': 1.3263311386108398, 'learning_rate': 2.834465700261198e-05, 'epoch': 1.55}\n",
            "{'loss': 1.2353, 'grad_norm': 1.4274406433105469, 'learning_rate': 2.6380650777612705e-05, 'epoch': 1.62}\n",
            "{'loss': 1.3393, 'grad_norm': 1.4694668054580688, 'learning_rate': 2.4408046661584408e-05, 'epoch': 1.69}\n",
            "{'loss': 1.307, 'grad_norm': 1.81044340133667, 'learning_rate': 2.2439128887084673e-05, 'epoch': 1.76}\n",
            "{'loss': 1.4237, 'grad_norm': 3.8856360912323, 'learning_rate': 2.0486158730277454e-05, 'epoch': 1.82}\n",
            "{'loss': 1.3315, 'grad_norm': 1.141914963722229, 'learning_rate': 1.856129815482759e-05, 'epoch': 1.89}\n",
            "{'loss': 1.2975, 'grad_norm': 4.033246040344238, 'learning_rate': 1.667653407425598e-05, 'epoch': 1.96}\n",
            "{'loss': 1.2536, 'grad_norm': 1.8412483930587769, 'learning_rate': 1.4843603704405279e-05, 'epoch': 2.03}\n",
            "{'loss': 1.2693, 'grad_norm': 1.3434035778045654, 'learning_rate': 1.307392147087777e-05, 'epoch': 2.09}\n",
            "{'loss': 1.2426, 'grad_norm': 2.1474545001983643, 'learning_rate': 1.1378507926623247e-05, 'epoch': 2.16}\n",
            "{'loss': 1.2186, 'grad_norm': 1.9901297092437744, 'learning_rate': 9.76792112233709e-06, 'epoch': 2.23}\n",
            "{'loss': 1.2765, 'grad_norm': 1.4531875848770142, 'learning_rate': 8.252190857053626e-06, 'epoch': 2.3}\n",
            "{'loss': 1.3113, 'grad_norm': 1.5674866437911987, 'learning_rate': 6.840756218384023e-06, 'epoch': 2.36}\n",
            "{'loss': 1.2854, 'grad_norm': 1.599857211112976, 'learning_rate': 5.542406801361758e-06, 'epoch': 2.43}\n",
            "{'loss': 1.3106, 'grad_norm': 1.0311321020126343, 'learning_rate': 4.3652279719506e-06, 'epoch': 2.5}\n",
            "{'loss': 1.3322, 'grad_norm': 1.6739474534988403, 'learning_rate': 3.316550516082137e-06, 'epoch': 2.57}\n",
            "{'loss': 1.2429, 'grad_norm': 1.56073796749115, 'learning_rate': 2.402904987779414e-06, 'epoch': 2.64}\n",
            "{'loss': 1.1126, 'grad_norm': 1.2518523931503296, 'learning_rate': 1.6299810406600419e-06, 'epoch': 2.7}\n",
            "{'loss': 1.2421, 'grad_norm': 2.7163074016571045, 'learning_rate': 1.0025919960785724e-06, 'epoch': 2.77}\n",
            "{'loss': 1.4132, 'grad_norm': 2.191526174545288, 'learning_rate': 5.246448685571365e-07, 'epoch': 2.84}\n",
            "{'loss': 1.2836, 'grad_norm': 1.2482023239135742, 'learning_rate': 1.9911603516855338e-07, 'epoch': 2.91}\n",
            "{'loss': 1.2423, 'grad_norm': 1.2500617504119873, 'learning_rate': 2.8032700388910814e-08, 'epoch': 2.97}\n",
            "100% 222/222 [11:31<00:00,  2.87s/it][INFO|trainer.py:3993] 2025-07-10 07:21:05,859 >> Saving model checkpoint to llama3.2_dora/checkpoint-222\n",
            "[INFO|configuration_utils.py:698] 2025-07-10 07:21:06,252 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3.2-1b/snapshots/9535bd9b1d1dea6acafbdc4813b728796aeb28da/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-07-10 07:21:06,253 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"head_dim\": 64,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 16,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 128004,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 32.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"unsloth_fixed\": true,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2356] 2025-07-10 07:21:06,321 >> chat template saved in llama3.2_dora/checkpoint-222/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2525] 2025-07-10 07:21:06,324 >> tokenizer config file saved in llama3.2_dora/checkpoint-222/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2534] 2025-07-10 07:21:06,324 >> Special tokens file saved in llama3.2_dora/checkpoint-222/special_tokens_map.json\n",
            "[INFO|trainer.py:2676] 2025-07-10 07:21:06,644 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 692.9645, 'train_samples_per_second': 2.559, 'train_steps_per_second': 0.32, 'train_loss': 1.3765611498205512, 'epoch': 3.0}\n",
            "100% 222/222 [11:31<00:00,  3.12s/it]\n",
            "[INFO|trainer.py:3993] 2025-07-10 07:21:06,647 >> Saving model checkpoint to llama3.2_dora\n",
            "[INFO|configuration_utils.py:698] 2025-07-10 07:21:07,029 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3.2-1b/snapshots/9535bd9b1d1dea6acafbdc4813b728796aeb28da/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-07-10 07:21:07,030 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"head_dim\": 64,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 16,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 128004,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 32.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"unsloth_fixed\": true,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2356] 2025-07-10 07:21:07,094 >> chat template saved in llama3.2_dora/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2525] 2025-07-10 07:21:07,100 >> tokenizer config file saved in llama3.2_dora/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2534] 2025-07-10 07:21:07,101 >> Special tokens file saved in llama3.2_dora/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  total_flos               =  2110677GF\n",
            "  train_loss               =     1.3766\n",
            "  train_runtime            = 0:11:32.96\n",
            "  train_samples_per_second =      2.559\n",
            "  train_steps_per_second   =       0.32\n",
            "[INFO|modelcard.py:450] 2025-07-10 07:21:07,336 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mllama_factory_run3_dora\u001b[0m at: \u001b[34mhttps://wandb.ai/adichats-2003-bits-pilani/llamafactory/runs/qybr1rwy\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250710_070933-qybr1rwy/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PiSSA"
      ],
      "metadata": {
        "id": "LGNvXEEYONsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pissa_args = dict(\n",
        "  **base_args,\n",
        "  finetuning_type=\"lora\",\n",
        "  lora_target=\"all\",\n",
        "  pissa_init=True,\n",
        "  pissa_iter=16,\n",
        "  output_dir=\"llama3.2_pissa\",\n",
        "  run_name=\"llama_factory_run4_pissa\"\n",
        ")\n",
        "\n",
        "json.dump(pissa_args, open(\"train_llama3_PiSSA.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "!llamafactory-cli train train_llama3_PiSSA.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "DxS91Lz-JhkC",
        "outputId": "5767aafe-1839-46bf-9813-642fb9537019"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n",
            "2025-07-10 07:29:27.282713: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1752132567.302867   21822 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1752132567.309008   21822 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-07-10 07:29:27.329387: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[INFO|2025-07-10 07:29:34] llamafactory.hparams.parser:410 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: torch.float16\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-07-10 07:29:35,278 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3.2-1b/snapshots/9535bd9b1d1dea6acafbdc4813b728796aeb28da/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-07-10 07:29:35,278 >> loading file tokenizer.model from cache at None\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-07-10 07:29:35,278 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-07-10 07:29:35,278 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3.2-1b/snapshots/9535bd9b1d1dea6acafbdc4813b728796aeb28da/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-07-10 07:29:35,278 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3.2-1b/snapshots/9535bd9b1d1dea6acafbdc4813b728796aeb28da/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-07-10 07:29:35,278 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|tokenization_utils_base.py:2299] 2025-07-10 07:29:36,258 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|configuration_utils.py:698] 2025-07-10 07:29:37,300 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3.2-1b/snapshots/9535bd9b1d1dea6acafbdc4813b728796aeb28da/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-07-10 07:29:37,303 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"head_dim\": 64,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 16,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 128004,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 32.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"unsloth_fixed\": true,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-07-10 07:29:37,662 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3.2-1b/snapshots/9535bd9b1d1dea6acafbdc4813b728796aeb28da/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-07-10 07:29:37,662 >> loading file tokenizer.model from cache at None\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-07-10 07:29:37,662 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-07-10 07:29:37,662 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3.2-1b/snapshots/9535bd9b1d1dea6acafbdc4813b728796aeb28da/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-07-10 07:29:37,662 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3.2-1b/snapshots/9535bd9b1d1dea6acafbdc4813b728796aeb28da/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-07-10 07:29:37,662 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|tokenization_utils_base.py:2299] 2025-07-10 07:29:38,355 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|2025-07-10 07:29:38] llamafactory.data.template:143 >> Replace eos token: <|eot_id|>.\n",
            "[INFO|2025-07-10 07:29:38] llamafactory.data.template:143 >> Add <|eom_id|> to stop words.\n",
            "[INFO|2025-07-10 07:29:38] llamafactory.data.loader:143 >> Loading dataset identity.json...\n",
            "[INFO|2025-07-10 07:29:38] llamafactory.data.loader:143 >> Loading dataset alpaca_en_demo.json...\n",
            "training example:\n",
            "input_ids:\n",
            "[128000, 128006, 882, 128007, 271, 6151, 128009, 128006, 78191, 128007, 271, 9906, 0, 358, 1097, 445, 81101, 12, 18, 11, 459, 15592, 18328, 8040, 555, 445, 8921, 4940, 17367, 13, 2650, 649, 358, 7945, 499, 3432, 30, 128009]\n",
            "inputs:\n",
            "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "hi<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "Hello! I am Llama-3, an AI assistant developed by LLaMA Factory. How can I assist you today?<|eot_id|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 9906, 0, 358, 1097, 445, 81101, 12, 18, 11, 459, 15592, 18328, 8040, 555, 445, 8921, 4940, 17367, 13, 2650, 649, 358, 7945, 499, 3432, 30, 128009]\n",
            "labels:\n",
            "Hello! I am Llama-3, an AI assistant developed by LLaMA Factory. How can I assist you today?<|eot_id|>\n",
            "[INFO|configuration_utils.py:698] 2025-07-10 07:29:39,104 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3.2-1b/snapshots/9535bd9b1d1dea6acafbdc4813b728796aeb28da/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-07-10 07:29:39,105 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"head_dim\": 64,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 16,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 128004,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 32.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"unsloth_fixed\": true,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|2025-07-10 07:29:39] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.\n",
            "[INFO|modeling_utils.py:1151] 2025-07-10 07:29:39,386 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3.2-1b/snapshots/9535bd9b1d1dea6acafbdc4813b728796aeb28da/model.safetensors\n",
            "[INFO|modeling_utils.py:2241] 2025-07-10 07:29:39,387 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:1135] 2025-07-10 07:29:39,389 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"pad_token_id\": 128004,\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:5131] 2025-07-10 07:29:40,260 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:5139] 2025-07-10 07:29:40,261 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at unsloth/llama-3.2-1b.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:1090] 2025-07-10 07:29:40,451 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3.2-1b/snapshots/9535bd9b1d1dea6acafbdc4813b728796aeb28da/generation_config.json\n",
            "[INFO|configuration_utils.py:1135] 2025-07-10 07:29:40,451 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"max_length\": 131072,\n",
            "  \"pad_token_id\": 128004,\n",
            "  \"temperature\": 0.6,\n",
            "  \"top_p\": 0.9\n",
            "}\n",
            "\n",
            "[INFO|2025-07-10 07:29:40] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.\n",
            "[INFO|2025-07-10 07:29:40] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2025-07-10 07:29:40] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.\n",
            "[INFO|2025-07-10 07:29:40] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA\n",
            "[INFO|2025-07-10 07:29:40] llamafactory.model.model_utils.misc:143 >> Found linear modules: o_proj,up_proj,gate_proj,down_proj,v_proj,q_proj,k_proj\n",
            "[INFO|2025-07-10 07:29:40] llamafactory.model.adapter:143 >> Using PiSSA initialization with FSVD steps 16.\n",
            "[INFO|2025-07-10 07:29:42] llamafactory.model.loader:143 >> trainable params: 5,636,096 || all params: 1,241,450,496 || trainable%: 0.4540\n",
            "[INFO|trainer.py:756] 2025-07-10 07:29:42,979 >> Using auto half precision backend\n",
            "[INFO|trainer.py:2409] 2025-07-10 07:29:43,436 >> ***** Running training *****\n",
            "[INFO|trainer.py:2410] 2025-07-10 07:29:43,436 >>   Num examples = 591\n",
            "[INFO|trainer.py:2411] 2025-07-10 07:29:43,436 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2412] 2025-07-10 07:29:43,436 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:2415] 2025-07-10 07:29:43,436 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:2416] 2025-07-10 07:29:43,436 >>   Gradient Accumulation steps = 4\n",
            "[INFO|trainer.py:2417] 2025-07-10 07:29:43,436 >>   Total optimization steps = 222\n",
            "[INFO|trainer.py:2418] 2025-07-10 07:29:43,438 >>   Number of trainable parameters = 5,636,096\n",
            "[INFO|integration_utils.py:832] 2025-07-10 07:29:43,441 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madichats-2003\u001b[0m (\u001b[33madichats-2003-bits-pilani\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.21.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/LLaMA-Factory/wandb/run-20250710_072943-pmvjrhcw\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mllama_factory_run4_pissa\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/adichats-2003-bits-pilani/llamafactory\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/adichats-2003-bits-pilani/llamafactory/runs/pmvjrhcw\u001b[0m\n",
            "{'loss': 1.5785, 'grad_norm': 7.048458099365234, 'learning_rate': 4.347826086956522e-06, 'epoch': 0.07}\n",
            "{'loss': 1.6359, 'grad_norm': 5.619606971740723, 'learning_rate': 1.5217391304347828e-05, 'epoch': 0.14}\n",
            "{'loss': 1.6203, 'grad_norm': 7.105066776275635, 'learning_rate': 2.608695652173913e-05, 'epoch': 0.2}\n",
            "{'loss': 1.7374, 'grad_norm': 9.40298080444336, 'learning_rate': 3.695652173913043e-05, 'epoch': 0.27}\n",
            "{'loss': 1.415, 'grad_norm': 7.039748668670654, 'learning_rate': 4.782608695652174e-05, 'epoch': 0.34}\n",
            "{'loss': 1.4743, 'grad_norm': 5.716498374938965, 'learning_rate': 4.9950171333287335e-05, 'epoch': 0.41}\n",
            "{'loss': 1.5098, 'grad_norm': 5.121190547943115, 'learning_rate': 4.9748082755013934e-05, 'epoch': 0.47}\n",
            "{'loss': 1.4277, 'grad_norm': 5.403594970703125, 'learning_rate': 4.939187749339435e-05, 'epoch': 0.54}\n",
            "{'loss': 1.3355, 'grad_norm': 5.6725640296936035, 'learning_rate': 4.8883773787879873e-05, 'epoch': 0.61}\n",
            "{'loss': 1.3628, 'grad_norm': 8.315935134887695, 'learning_rate': 4.822693581319326e-05, 'epoch': 0.68}\n",
            "{'loss': 1.3675, 'grad_norm': 5.9479241371154785, 'learning_rate': 4.742545397468659e-05, 'epoch': 0.74}\n",
            "{'loss': 1.3862, 'grad_norm': 5.576938152313232, 'learning_rate': 4.64843194356728e-05, 'epoch': 0.81}\n",
            "{'loss': 1.4508, 'grad_norm': 5.628730297088623, 'learning_rate': 4.540939303535998e-05, 'epoch': 0.88}\n",
            "{'loss': 1.374, 'grad_norm': 9.231779098510742, 'learning_rate': 4.420736879094927e-05, 'epoch': 0.95}\n",
            "{'loss': 1.3317, 'grad_norm': 6.1010637283325195, 'learning_rate': 4.288573221118428e-05, 'epoch': 1.01}\n",
            "{'loss': 1.2918, 'grad_norm': 6.691281795501709, 'learning_rate': 4.145271368095094e-05, 'epoch': 1.08}\n",
            "{'loss': 1.0724, 'grad_norm': 8.33768367767334, 'learning_rate': 3.9917237207221406e-05, 'epoch': 1.15}\n",
            "{'loss': 1.1935, 'grad_norm': 4.843595504760742, 'learning_rate': 3.828886484552254e-05, 'epoch': 1.22}\n",
            "{'loss': 1.1091, 'grad_norm': 4.803024768829346, 'learning_rate': 3.65777371530087e-05, 'epoch': 1.28}\n",
            "{'loss': 1.2605, 'grad_norm': 5.857323169708252, 'learning_rate': 3.479451003896233e-05, 'epoch': 1.35}\n",
            "{'loss': 1.3201, 'grad_norm': 6.6815505027771, 'learning_rate': 3.2950288405980956e-05, 'epoch': 1.42}\n",
            "{'loss': 1.2185, 'grad_norm': 6.407329082489014, 'learning_rate': 3.105655699509458e-05, 'epoch': 1.49}\n",
            "{'loss': 1.0271, 'grad_norm': 5.7372331619262695, 'learning_rate': 2.9125108865469987e-05, 'epoch': 1.55}\n",
            "{'loss': 1.1164, 'grad_norm': 6.839916229248047, 'learning_rate': 2.716797195408878e-05, 'epoch': 1.62}\n",
            "{'loss': 1.194, 'grad_norm': 5.707584857940674, 'learning_rate': 2.519733417274284e-05, 'epoch': 1.69}\n",
            "{'loss': 1.1881, 'grad_norm': 6.934677600860596, 'learning_rate': 2.32254675087996e-05, 'epoch': 1.76}\n",
            "{'loss': 1.2894, 'grad_norm': 9.434785842895508, 'learning_rate': 2.1264651602393477e-05, 'epoch': 1.82}\n",
            "{'loss': 1.2147, 'grad_norm': 4.904485702514648, 'learning_rate': 1.9327097275960243e-05, 'epoch': 1.89}\n",
            "{'loss': 1.1735, 'grad_norm': 13.086133003234863, 'learning_rate': 1.742487049232818e-05, 'epoch': 1.96}\n",
            "{'loss': 1.0696, 'grad_norm': 6.297966957092285, 'learning_rate': 1.55698172149106e-05, 'epoch': 2.03}\n",
            "{'loss': 1.0285, 'grad_norm': 4.932338237762451, 'learning_rate': 1.3773489637926995e-05, 'epoch': 2.09}\n",
            "{'loss': 1.0175, 'grad_norm': 7.605870723724365, 'learning_rate': 1.2047074246048063e-05, 'epoch': 2.16}\n",
            "{'loss': 0.9932, 'grad_norm': 5.633517265319824, 'learning_rate': 1.0401322151467341e-05, 'epoch': 2.23}\n",
            "{'loss': 1.0442, 'grad_norm': 5.517959117889404, 'learning_rate': 8.846482142219752e-06, 'epoch': 2.3}\n",
            "{'loss': 1.0775, 'grad_norm': 5.160640239715576, 'learning_rate': 7.392236858683194e-06, 'epoch': 2.36}\n",
            "{'loss': 1.0309, 'grad_norm': 6.370980739593506, 'learning_rate': 6.047642495718867e-06, 'epoch': 2.43}\n",
            "{'loss': 1.0707, 'grad_norm': 4.431451320648193, 'learning_rate': 4.821072405950369e-06, 'epoch': 2.5}\n",
            "{'loss': 1.1003, 'grad_norm': 7.06110143661499, 'learning_rate': 3.7201649553876366e-06, 'epoch': 2.57}\n",
            "{'loss': 0.9975, 'grad_norm': 5.527383804321289, 'learning_rate': 2.7517759561204948e-06, 'epoch': 2.64}\n",
            "{'loss': 0.8627, 'grad_norm': 4.7897210121154785, 'learning_rate': 1.9219359723034845e-06, 'epoch': 2.7}\n",
            "{'loss': 1.0067, 'grad_norm': 9.029827117919922, 'learning_rate': 1.2358127653054303e-06, 'epoch': 2.77}\n",
            "{'loss': 1.1528, 'grad_norm': 7.262712001800537, 'learning_rate': 6.976791118935761e-07, 'epoch': 2.84}\n",
            "{'loss': 1.0626, 'grad_norm': 5.229697227478027, 'learning_rate': 3.108861958619064e-07, 'epoch': 2.91}\n",
            "{'loss': 1.0163, 'grad_norm': 4.477662086486816, 'learning_rate': 7.784273880517002e-08, 'epoch': 2.97}\n",
            "100% 222/222 [04:21<00:00,  1.10s/it][INFO|trainer.py:3993] 2025-07-10 07:34:06,306 >> Saving model checkpoint to llama3.2_pissa/checkpoint-222\n",
            "[INFO|configuration_utils.py:698] 2025-07-10 07:34:06,734 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3.2-1b/snapshots/9535bd9b1d1dea6acafbdc4813b728796aeb28da/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-07-10 07:34:06,736 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"head_dim\": 64,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 16,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 128004,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 32.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"unsloth_fixed\": true,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2356] 2025-07-10 07:34:06,801 >> chat template saved in llama3.2_pissa/checkpoint-222/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2525] 2025-07-10 07:34:06,806 >> tokenizer config file saved in llama3.2_pissa/checkpoint-222/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2534] 2025-07-10 07:34:06,807 >> Special tokens file saved in llama3.2_pissa/checkpoint-222/special_tokens_map.json\n",
            "[INFO|trainer.py:2676] 2025-07-10 07:34:07,124 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 263.6858, 'train_samples_per_second': 6.724, 'train_steps_per_second': 0.842, 'train_loss': 1.2299073343878393, 'epoch': 3.0}\n",
            "100% 222/222 [04:22<00:00,  1.18s/it]\n",
            "[INFO|trainer.py:3993] 2025-07-10 07:34:07,128 >> Saving model checkpoint to llama3.2_pissa\n",
            "[INFO|configuration_utils.py:698] 2025-07-10 07:34:07,478 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3.2-1b/snapshots/9535bd9b1d1dea6acafbdc4813b728796aeb28da/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-07-10 07:34:07,480 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"head_dim\": 64,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 16,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 128004,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 32.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"unsloth_fixed\": true,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2356] 2025-07-10 07:34:07,527 >> chat template saved in llama3.2_pissa/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2525] 2025-07-10 07:34:07,532 >> tokenizer config file saved in llama3.2_pissa/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2534] 2025-07-10 07:34:07,532 >> Special tokens file saved in llama3.2_pissa/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  total_flos               =  2109864GF\n",
            "  train_loss               =     1.2299\n",
            "  train_runtime            = 0:04:23.68\n",
            "  train_samples_per_second =      6.724\n",
            "  train_steps_per_second   =      0.842\n",
            "[INFO|modelcard.py:450] 2025-07-10 07:34:07,765 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mllama_factory_run4_pissa\u001b[0m at: \u001b[34mhttps://wandb.ai/adichats-2003-bits-pilani/llamafactory/runs/pmvjrhcw\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250710_072943-pmvjrhcw/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SDJw7k1uOc66"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
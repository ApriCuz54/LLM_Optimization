{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q82u9AFQ0iBy",
    "outputId": "ebcf7ae1-0374-47ce-dadc-0310340ec74d"
   },
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install --upgrade peft\n",
    "import torch\n",
    "import time\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8esnT40IORS"
   },
   "source": [
    "# TESTING BASE MODEL WITH NO OPTIMISATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zPRyN3FC1EnK"
   },
   "outputs": [],
   "source": [
    "def measure_resources(model, tokenizer, dataset, training_args):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Start memory tracking\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Training setup\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "    tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "    tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset['train'],\n",
    "        eval_dataset=tokenized_dataset['test'],\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # End memory and time tracking\n",
    "    peak_memory = torch.cuda.max_memory_allocated() / (1024 ** 2)  # Convert bytes to MB\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "\n",
    "    return peak_memory, training_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 859,
     "referenced_widgets": [
      "9ebb3a59d32e4dc8a990c7d918a5688c",
      "16543920390b47dfb83e4fdf8185e5fe",
      "1d55bec64c6a40d49bb8435af6a2ad9b",
      "690f4f9f17f241d1be6d23b5a98d3c4a",
      "8a520b9ade08479da16655f984455aeb",
      "6fa80aac19d84d51ba6ce777467314ff",
      "acb5de4e111442ffb42862cbc468e77f",
      "4a4a99cf0f3649389fa68bcecb436fc2",
      "7a3ace8036b54b2582a94754fdd461b2",
      "888a48b1858a4d6796529546cf5a715e",
      "ebfdd8cc9080444e9b1c42cd764294eb",
      "244e85ff149a460aadbc2bc86b81b0dc",
      "83d013fff8af411ca21acbcc43a333cf",
      "79ae996ff95a464dbd33cca8244a5eea",
      "eb2534a902da4ee6a69038a763203718",
      "2b707dce3c974fc096ccc291a8d6e57d",
      "4f4f2e1379d04c888c4e054c38526def",
      "8fb342cd02f24e9a923eb7a252b109ce",
      "3ee73423f6124c099854dc17bb2a4093",
      "58859fdd5fb8402a9be99420b9dd226a",
      "54bbf3812cfc428e9a73df0dadf4cb14",
      "2e4b0cf7b041464b83d59d2fff2a4d56",
      "0bcf69d784f640e6b7fdd9a0a7ef84b1",
      "0b7f1759d2ef4507b46092dce8f2aa61",
      "b76e410028874e29be000c7eb3d14435",
      "cc50e8a1c81546c3994acbe2a9f61a5c",
      "c821a2c64aec43cfa07e2bb643639c96",
      "ea52ede6a89b41e9ac6d48188242cbe1",
      "1e9ff05a064d45f6ad4f8f8d0ebdea3e",
      "c6be4557faad4d5e886a68cbe6d38860",
      "fb0f556311614d2abb5917d1aa2faa20",
      "7e7363f877044beeb509190d70fefa29",
      "88e188f7ac5e4b1391425514a90859c9",
      "b69d93e71f0e412dab994bd245001983",
      "134198ac836f4f8da96a447b5ebb5702",
      "36ebc14b0eae4c9c95df56eb50e501a4",
      "62a82fa100ce4c90a3b9601aaa058849",
      "96b8b998144a4071ab6a02c0c28a6c82",
      "ebfae0a0ce804e79adfa74574a8628fa",
      "8ab4581ca965465aa6374c50f95ba35c",
      "8b704ea1894c48b6a89b9d15e3fd62a5",
      "a8bf47e8a3f344a5a9798ec614e5f28a",
      "b284855e0cac4dca99ba39d05d0e89f2",
      "5cef387044704f149b174729038a106b",
      "03bcf3d59f1f4764b77b040004267fcb",
      "3044152c78e242aebc83ada4a7d15e38",
      "2347590c97de4e92aab87ee6f135d108",
      "fdf3a0c2b22e4854b1aa6dcd88c6ce25",
      "1f2eeefd5122407d90fa130e8b3fa87f",
      "dc718e10909f496589d152d4ed1c1dc7",
      "6c0a9495670545c88a118c10d050c42f",
      "fea76e9b27dd47a898fe987b46ddd850",
      "81d69955ba27422788b94c138b4215c3",
      "962e4ade22a44f74a2517cfcc7f2a47e",
      "7701dd8e59704c5b81ca510b7f462a50",
      "0c305ce0ef44400f8b5d4658918b9dad",
      "c52e50c69a82425f80af85384d91e5fd",
      "2ddd317c618b49bca8ee64bbcb4bb5dc",
      "2088c2bc8f7b446bb009ccfe74e548ef",
      "e968eac8520e450eb0ed4d000eeef9c1",
      "0b498e46a03b44569765fc3119fd3503",
      "5e002bbd73f44d278357fb3915d48c51",
      "90d764d1ca1344ca921a07bc66073c92",
      "701b5ef4e13f4fe3b4b6ef3714925330",
      "2ac5469e9eb04f4686192b07bb46f657",
      "af3f9d2b6a4b4b699d3d9f0205b916fc",
      "680d5817b8494e20aa255800afd5201d",
      "37a843939ac2452eb77fb1c37bc1798f",
      "efd2bfeb17114e7fac5082c5b08f1d17",
      "22b269fd164047c4bdb4a797319db6fa",
      "f82c211779c0446487bc8965f2f206c4",
      "55cf0c50709a4da9926478074e10e8f0",
      "0c7715cbeea3407da6287995128d0d71",
      "3320722e804b46ff88bb753faa8e6cca",
      "7ba9d5d977254b2eb6d30878ed8e1c9d",
      "5a07a02d85464abbbbcb1d36de6f9092",
      "53341eff774c4425b604982379793df2",
      "b407daa5fca14d5c92ba51f5577615fc",
      "e640f3ae90ca4d06804eb9422d542a16",
      "5ca497c1db4349f7b581e1db39f8588f",
      "a2f4f15543504c11801b7c509571e04b",
      "4ddfa68d8f87418ebce4edb018cbbc8c",
      "80b10e0c44fc4d5280d6f7dfbc5eb873",
      "6c666e425f574b5589d279b28aaad430",
      "a62cfff58ee743d381fa9ac1ce569822",
      "310b84c9297340e08116995ba0a20218",
      "257c5614132c44e8844dd6867333d2aa",
      "7e32b8175d72493f9f1f2714a3760220",
      "e9e5ab805474461eaff0b6c80de71a81",
      "46d3d106ffcc4871a39d6e596ed959b1",
      "f8ee003577ee40ddb0fb2b2c3d48dbe0",
      "882b43f4c5c34a2bb20b0d1b47bf8876",
      "2377a66438dd480ab99be3726f3a3277",
      "7cce592d588442ab88014d36df6c0607",
      "7962988eaa884d53b2da265689946f1a",
      "629ff8fa05e14bca96d062aebaed252e",
      "d2951c17551c48c28344138cd8ace833",
      "307f326c426e4670bb7657982c81f0f7",
      "b24c5336243542868b9818bb51d7170f",
      "88e05cd7f5fb44699ce86b0b72f6119e",
      "f48434638e7d48c18748a687a701b99c",
      "04d7c2408e544f62a4d3ad908395b452",
      "3cba22e0538b44c392adbc7b5fe4f5aa",
      "da24b94bea2f44678780c605b7a3b0b6",
      "526450bf479644588b6ab6ab54e1beb9",
      "f487a927659240f7823d9f155b723496",
      "becd58e924af4ef481f209c5f5ab7ef9",
      "364419c6984c498bb7f30170d0c12e3e",
      "2e53859ef47447d3b42f305eb61ad97d",
      "f68c8f90a2d448e9a72a525441f7f326",
      "c68b382f863d4e75833242d694146b0a",
      "55ee12fe21c443ef91a1246b58ed9e9a",
      "ee46a31b16484463b0fd06ae77f08c90",
      "8f78450ea2934b948491c803fa102d53",
      "9c8dedf53bd24e6f957978fdfe9488d4",
      "0369f88b68944cb2be878e2fef8e9a0f",
      "ec5d7517a01e4406b3aef3e6799bd28b",
      "38ce39e3b7114817b2449dc09d3c07d4",
      "1fae3fee047d41f7b9817a61b5199040",
      "576b1caccc724881bf2e365840243e2f",
      "f41638750b5e40bbbfb35c5b200c1288",
      "176ec423e3664ff1b2017e1aa469cc43",
      "53300a7f3f6b42e582f7fe102c785a38",
      "3ccceb81e4664c668f5572751aa4d9b8",
      "8cd33145380a476da55ac8ee5a70494b",
      "f9a7194cee0c41298197451582de8df9",
      "d7896a6153b2481ea982564a008c1e61",
      "cdd73afa19d34aad87965ab888d58f9d",
      "b2659d34b62f47bb996d494b39d6c7c2",
      "04c8f0f078f34bc0bb1eae2f578be14b",
      "8900405f5b554d91bcb7040322bbe9c2",
      "9a3bf3f2432b4664ae02878e7f631e5d",
      "18293d9ad8f947488b80ef4975f46b41",
      "976cc183b22440628e5d5cb130814aa5",
      "543cbebbb35f489a9b60958d146c76cb",
      "0da50e7318204f44963f21821fa1cf84",
      "f2eb49a0751847d488f98857641c2bb8",
      "4321f2e07785457d8d2f8732ff7fc5b1",
      "c425f92b98b940dcba3ed4232287a7c7",
      "d80c3ac4d5314ef5a0d43dba19e34896",
      "3e7bab29441c4ff387fd8a46da04662d",
      "edbf029cc08d4cab87a5c7e1ae20a82b",
      "0b54e8c0cdba46bba6eef8439b73204f",
      "be6860d81eff45fe92bc2f7aab43a6c5",
      "1bd8846adf8047d8aa1c609fa6a17597",
      "c4d2db3ca5a24bdd932c54883ad3138e",
      "1991c9c95ef64cda98927bb23dfd2fbf",
      "9a41587392d94f36aadb4a8ab41df5db",
      "daa8bcf227d84b8d9df110bad7227e85",
      "19c0419369994a708895a4e3fa03cfcf",
      "436d96406d204fc4a30f62dab80a0c23",
      "f2c2f647887b4bb880268837bf01e434",
      "e0a86e22b53045e2a4686ffaa4975157",
      "8051f8d3c96149878c4e721f7d0472da",
      "0c0b105461d2434d8631002eca510d51",
      "aad74b4125f241a18a5fbc2abcb6b109",
      "97f204e047b74dd9a1b30205efff9b06",
      "e6241a8acf3c46bd8bf8fe37ac5afd8e",
      "947d07b14d364076838c7ddf1a57407b",
      "ade29df76a4d4c2988e73bc0406a21c3",
      "7bdc8b9f7fcf43e49d8c65373ef06298",
      "607a19862bbb4818ae1ff842b74f504b",
      "6b686d6d36a249feba6693e9a4ce4c0c",
      "2daa34565cd8476b9c9f8e050efe3841",
      "16dcfe18ea7549bfa2c254b04f720fe8",
      "c79cf620221c491ca8a79d19afce4d56",
      "8ba32deef4f94c8d8bbc42334595e27f",
      "ba47d4249bf447f08986d5a81f3b974b",
      "eb6b77ef4e9542108218fd95cce565ae",
      "c78d4b7bbed34cd18fa1064d2080faf0",
      "ddd3deda5dfd4c0d98b933c71469fa8a",
      "3ca8783c68004affb5e757eedf009e8f",
      "defc6a1e877d45c3b8a33656e34fd20d",
      "4ca20dfb700a44a494280a3f2a634c88",
      "661cac56dc674e2bb3a3906c244d09f4",
      "51b6393c461e416aa45be33e473c820d"
     ]
    },
    "id": "crU7N-hG1vIu",
    "outputId": "c995012f-7006-483d-ddb2-85bc3684fbdc"
   },
   "outputs": [],
   "source": [
    "# Load the dataset and tokenizer\n",
    "dataset = load_dataset('imdb')  # You can use any other dataset\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=16,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=1,  # Set to a smaller number for testing purposes\n",
    ")\n",
    "\n",
    "# Measure fine-tuning without LoRA\n",
    "model_base = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
    "peak_memory_base, training_time_base = measure_resources(model_base, tokenizer, dataset, training_args)\n",
    "print(f\"Without LoRA - Peak Memory: {peak_memory_base:.2f} MB, Training Time: {training_time_base:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BoZxr-CeITTh"
   },
   "source": [
    "# TESTING BASE MODEL WITH LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 157
    },
    "id": "eWt5bjmT1zkm",
    "outputId": "09bd92f8-788a-40bd-9fcd-d2380bf881dd"
   },
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,  # Sequence classification task\n",
    "    r=16,  # Rank of the update matrices\n",
    "    lora_alpha=32,  # Alpha scaling factor\n",
    "    lora_dropout=0.1,  # Dropout rate for LoRA\n",
    "    bias=\"none\"  # Bias strategy for LoRA\n",
    ")\n",
    "model_lora = get_peft_model(model_base, lora_config)\n",
    "\n",
    "peak_memory_lora, training_time_lora = measure_resources(model_lora, tokenizer, dataset, training_args)\n",
    "print(f\"With LoRA - Peak Memory: {peak_memory_lora:.2f} MB, Training Time: {training_time_lora:.2f} seconds\")\n",
    "\n",
    "# Compare Results\n",
    "memory_savings = peak_memory_base - peak_memory_lora\n",
    "time_savings = training_time_base - training_time_lora\n",
    "print(f\"Memory Saved: {memory_savings:.2f} MB\")\n",
    "print(f\"Time Saved: {time_savings:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0dpjvcQjIaHf"
   },
   "source": [
    "# MIXED PRECISION TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "id": "cDJ8jrE86aC6",
    "outputId": "cf5f7c05-4355-4cf2-90f2-db2ae13df6a9"
   },
   "outputs": [],
   "source": [
    "# Training arguments with mixed precision\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=16,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=1,\n",
    "    fp16=True  # Enable mixed precision\n",
    ")\n",
    "\n",
    "# Initialize and train model\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
    "peak_memory, training_time = measure_resources(model, tokenizer, dataset, training_args)\n",
    "print(f\"Baseline with Mixed Precision - Peak Memory: {peak_memory:.2f} MB, Training Time: {training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E87i23LJrPI9"
   },
   "source": [
    "# PRUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375,
     "referenced_widgets": [
      "a8b15d0dfbbd4cd78e41d2ef35d35297",
      "1ec72bdee60b4da99842ec1f4fe21e8c",
      "ef833862d8dc4e058a1a10d4df6581bc",
      "d37f8671de2e4322970ab1b92b683881",
      "60bd1ed04fbb48b5935cdc148f7a592d",
      "378dc17a2da44f86826f0c1133f62542",
      "74096b62aa1e4ebd91e89341e92ae882",
      "538541d03c84452682558589c830d4c1",
      "a5944d559efd45bb808194c2b0edd8bd",
      "8c4156b5b2fb419897270f3d587cbd40",
      "18ee65d69dbd4cefaf39e6ccb13aa7b6",
      "67db3588e728400ea0d15daad169a3ae",
      "78cf0a43408947109dfdfafc95284378",
      "8d417d714d93471f9da2a4286852ac38",
      "42907ca807ab4a62abd73f2806b7f099",
      "b3d3ef4c50c44590ba57965f9f5a70d5",
      "c9c0190c28744b21b871acd80b15d068",
      "a763c75c41d648cfac1517ee07983ac0",
      "93bd7c8afd044d5a92d637e0c93e75b0",
      "b8558efe081843e38327b247396dccc0",
      "372c4c2ee8ca42e89e65323a2c63935d",
      "35c69287696b40f0a5d4d3714e8b1e5a",
      "ac9836fc94e540439add4d94c06b5498",
      "662e55a038324d09872e032e9c6aff4e",
      "769d1a4d7b64459d801cd68e7ecb40c0",
      "04905d7f919e48f89de689f2b0662c31",
      "68f3ae38f6c743af98f9b881297dee95",
      "562d4b44a23d473eb25db26c712518e0",
      "f17f3f2c36174e27b8cfd964df8aef2d",
      "4b773cee3b9142b9b628519af4268fb2",
      "2a6c74981fec406aacc31bf6ea4459eb",
      "f3b16d328b174998be72fef9557d390c",
      "cbd9bf74ce8a474f8e0f8103e5539e4e"
     ]
    },
    "id": "gZBoj4SUUhk5",
    "outputId": "e7891c46-1b8d-44b3-9d0c-c74fe96452b6"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.utils.prune as prune\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "\n",
    "# Define pruning function\n",
    "def prune_model(model, amount=0.2):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            prune.l1_unstructured(module, name='weight', amount=amount)\n",
    "    return model\n",
    "\n",
    "# Initialize and prune model, then move to GPU\n",
    "model_base = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
    "model_pruned = prune_model(model_base)\n",
    "model_pruned.to(\"cuda\")  # Move pruned model to GPU\n",
    "\n",
    "# Measure resource usage for GPU-based model\n",
    "def measure_resources_gpu(model, tokenizer, dataset, training_args):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Start memory tracking\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Tokenize and prepare dataset\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "    tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "    tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "    # Adjust to_device function to handle only numerical data\n",
    "    def to_device(batch):\n",
    "        return {\n",
    "            k: torch.tensor(v).to(device) if isinstance(v, list) and all(isinstance(i, (int, float)) for i in v)\n",
    "            else v.to(device) if isinstance(v, torch.Tensor)\n",
    "            else v  # Leave non-tensor, non-numeric lists (like strings) unchanged\n",
    "            for k, v in batch.items()\n",
    "        }\n",
    "\n",
    "    tokenized_dataset = tokenized_dataset.map(to_device, batched=True)\n",
    "\n",
    "    # Initialize Trainer with the GPU-based model\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset['train'],\n",
    "        eval_dataset=tokenized_dataset['test'],\n",
    "    )\n",
    "\n",
    "    # Train the model on GPU\n",
    "    trainer.train()\n",
    "\n",
    "    # End memory and time tracking\n",
    "    peak_memory = torch.cuda.max_memory_allocated() / (1024 ** 2)  # Convert bytes to MB\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "\n",
    "    return peak_memory, training_time\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=16,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=1,\n",
    ")\n",
    "\n",
    "# Load tokenizer and dataset\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "dataset = load_dataset('imdb')\n",
    "\n",
    "# Run the pruned model on GPU and measure resources\n",
    "peak_memory, training_time = measure_resources_gpu(model_pruned, tokenizer, dataset, training_args)\n",
    "print(f\"Pruned Model on GPU - Peak Memory: {peak_memory:.2f} MB, Training Time: {training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "762VvFFRRlAO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

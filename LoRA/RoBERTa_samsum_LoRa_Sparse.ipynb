{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8fko_A-LkvKL",
    "outputId": "e2462eb6-50ec-4ba4-c7ae-e68fac3a30c5"
   },
   "outputs": [],
   "source": [
    "pip install transformers datasets peft accelerate py7zr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 947,
     "referenced_widgets": [
      "3a7a16b18093429ca3f09aea61afdb8d",
      "ba7f3ecc13a145b8ad82c85e23f96ad3",
      "365349e224e34480bd0790be26046759",
      "58945bc7e8084c19b0ee036a439f4c2c",
      "0b39252c6f9c410ca584be84352d64c4",
      "7e840af54de84328b271bde42b927870",
      "6f3524e3dbc34182ab662a6a73fd89a5",
      "28438d8a2e3d49e1bb6c9836b9469f33",
      "185938edddbe4ed0b6d052dcbb3d6fd9",
      "dc58b4ab16324c609f4db3e5a15dec82",
      "f5ecb23642e743b790a718e7c386d5fd",
      "96eb6232efcf4de6ae652b10cba3a721",
      "dc481cd0f58840d2a19237df3e4e8197",
      "0f8d72bbed6e45ad89c578ddca53624c",
      "eca7fdfaeab74005820291b28bebda74",
      "f5dbef2e021a4f0f8e1ebf739db9dedd",
      "313bf545762c4ce6853cbcaf2453429c",
      "000f12f8b4a54c868451854a108db818",
      "a56ab88bc5c745d2bd63a80649ec9886",
      "eb62eb8d155041b28bf58b96da4a17be",
      "0526e86237df40eb811a30cd16f9454b",
      "83aae196bb5549d59bd8d9a140e14c79",
      "071bdc12c1934a8581a191c98195b768",
      "c457049323594ad698bad1b464467364",
      "02d6cffb3aab4608a1a647c507fb5e6c",
      "138aa288bda143e889f35b16ae6ac7f1",
      "ebbaf43596544f54b9dc98fdfce3cb68",
      "87baf571a9a64f7d8c1104b0bc706e09",
      "30e15d7c981d4d0fae2e6c5f448a1fcc",
      "8891c50d888a43efa4eca92669aea055",
      "729e5015cd3443f2bf29c29e2e5c2331",
      "270e6321007e4adeaca45dfbdd92a285",
      "8689f0c8c953436a95999639001c5190",
      "01f908b2dd1443b7b5d8a83c2d314c75",
      "d034d131bd9b47ed89ffcc0645c18e54",
      "b4ffdcdd0d15449bafe4ddae191fb862",
      "a85dac7b5a964297903a89407dd29702",
      "31a6cce2c0e64177af4068b1d7193490",
      "8021686c9d2849e1a8a584972a082b3a",
      "ec7442f7847047119708cc11d4f90775",
      "cbdf46aee29c402198634cee25e14256",
      "b6bbc854000e4f9d80b6f2d622496283",
      "f2541f4139c74daa8af79a2b70c064d1",
      "e481846390424484b695e22f45d9b329",
      "2c90abe8d189456bb7f4373a47025c04",
      "38dba883579a413280beb4d32e93a50c",
      "764b8e7729c5458686bb365101bbc36a",
      "172dcb57c82f4b04a9f39863a9b56bd5",
      "9c7950bd42194fcd853635150ac51c97",
      "7c77ef49feb7405b8a365690e2fdcd82",
      "5876becfa41e474c9cf7cd8bad8ee516",
      "7ac07b53a1614eadbc17362154cb566b",
      "b571dcf929344810b92394fa60a9163a",
      "8e0649fc62a64987a66c222852e5c59a",
      "4f29f38403234558806d5823718a2911",
      "270aa8b977e84fa086d9784fc3a0d0f1",
      "fb15f5cb713740ac83e98ec4f9dcfc33",
      "850f4339ce8d40a6af4ba7276ae0e023",
      "b9856d690f994f40aac1725438593266",
      "57a7c9849a944164a2509e17c6136935",
      "d5d71171c8fa4cd79e9cb0a4e4787a4b",
      "a5450f7fbc33408d9080ce0e405c3525",
      "f27d707166ae47aab239d110ec8897e3",
      "608df97689424dacbfae2968c96ac11c",
      "086bb79e36a0460fa8fbce59c996c717",
      "d77b4066d53a4cdfb9f5e9f51448dc9d",
      "b6800099f57e4635bcf85b5777603065",
      "2a1a5fd4fa83451a8343da90972e4322",
      "7d6afd50a4124f69a23d158f7306a1cf",
      "1f3539e9a5cf42a8959e417e27c470c3",
      "e3f4906ef1f54c24bb30ccee62e78769",
      "cf5c5c630f7e4f5690033e23238d9f6a",
      "9ee9114d068b4eb99b1bee4fa6eae64c",
      "2f8678cbb7044cd6acba231a2d5bf7b8",
      "ef498a7f100e45a39be47f8c438a3521",
      "3e03f3d94d0a4e5e9eff1abe27398cd7",
      "c5398767d8384b92bb25f6f6ba2a17ec",
      "edecec63a3dd487fbbf655d5015397b8",
      "f5d7514ada7a4121a29f02ab702d872e",
      "17dd8cc173c941759130f19754008b84",
      "7398ffcab36849a48f9f3d74d6663bd5",
      "c2cb4bd8868e4ce3bbaf6f2398b1f461",
      "f0d59324712a4907a73c94d12160610a",
      "0b286c880cc5478091b8e8c95f69874a",
      "ca832026a1c1495aa2eee9342cff767b",
      "a4a1224f3920424d840ef46be166237a",
      "c220331b1ce94da28d15e8c371789935",
      "cbb3c044a1a24b40baa9d92969272f53",
      "ba27ed6ed1ee404395da9134af958a3e",
      "57619237f90a411a8179140ce3504da7",
      "ca45b28fe5c64c7c92050197854d942b",
      "1683a6b4c99743a2b7b02f38c2f370a1",
      "d23808319bbe4b76a037298261bdeba8",
      "7adc2c0c06364f2ab90acc3504965fec",
      "99d4c207355e4512a01aadb8f0331d9d",
      "72c78863a78e4e90bdaa815cb265e3d0",
      "d2085e2514bb409c9da16a4bb6534d97",
      "887130bfb2ab4de98969aac7fd3a5b15",
      "b2811d1d7f6a4aa19197acc09aaf7f5f",
      "915d1abb8f24441ca792118f1d35b1cb",
      "0acee8aef8d143fb91ffab2aea14e3f8",
      "a88269904a7d4946a68934ad8648d40e",
      "b0b4590d6d714cd497929dc4e8fd3639",
      "abcca3808989444ebb0d1fcc3e53ec50",
      "95dfd5befe834c089783c1b58a7b163f",
      "214702a3afaa4c68876adeb34882a8fa",
      "d965f0c3338e48399a44cc84e0fea65a",
      "c396a4539f8c4bc1b7d49817b61b9470",
      "748119aa689748d2978d60ef04bdd2f2",
      "8cfea19c61cf4c209d2fe9b1d25b86b4",
      "a1d5fb2fc25541dfa82f6a42cd4b9fe1",
      "4c776f6e7b6344828a3767a6298e6929",
      "01813006c2a3424ab140720a68efa33a",
      "b37f6bb083ac43a7b96c7bf696cc6f43",
      "1ca39920b4ba48cf944310932f17719f",
      "a27f3101a5c940f189c3c9647b9e534a",
      "7b7eb8b572ba4e0b8c510f658c6834f8",
      "59b5a83936c942b3bb6360b204f899e9",
      "00ad59770d804a3aba7926e4092366dc",
      "c9199009afb5456ea9729a789ed8ed44",
      "a309c849bb534f788a1c77d8054a0abc",
      "34d2a0472cc44295adc48ca3b4b06545",
      "f9b49d40f6a34d90aeb0fffc5b3b93a5",
      "667c5f4299c14f6f99f6e2cde27dca4f",
      "fdbaa0a9ef414989ac0b44797a6e7902",
      "4795b1b7351748aa89ebac2089cce057",
      "26d701ead26b46bdb9f40e58f708cf32",
      "15fbd314206945ccacb746c0b78da5e5",
      "ff3d37dfeced4862be84e5e81e1b426e",
      "e5abdb3601594f87af8a0818e3ff7118",
      "1e8ed6ac6fb3491b9b4d9087574b58a5",
      "ccecc1cb2cfb4e7284f6eae0a247c558",
      "8724fe41fc3f42a18827e0725299c020",
      "d9bb882c6f244db781ffa69d4a312547",
      "096872cabf744279bbfdd7868e14dcb1",
      "c58a11424b3a4836baf343b0eff5bf03",
      "5822a92f69814fe3803beda90b641557",
      "0be645386e5740fab9f248918486e686",
      "4f11aa1072ef4d2e8b4693fcabf1267b",
      "b9c44cb1f8bc439b89b32d61b1ccbcdc",
      "d311caf272dd425aa71821d36604a427",
      "0dd3383a8dc34d83b92cc368a2e5db82",
      "d7f96c28b89e4e7190a6b033ebeded42",
      "8d4bbcab18cc40569791fa22ece0fa0f",
      "4e50a662dd3145b789fb098012b433bc",
      "bb03c933c95c404bb77cbfc316a74ec1",
      "7edeff0aee4142a7829e3a68db131ac4",
      "11a9b1f5ed4e4c4a9f6f1a5cd40823dc",
      "55028a40d6ab4e468ad2582bdcc35052",
      "b6be8b726a0e4cdaa98875986e496ee2",
      "bcb248e363ea471abbe15e3bcec0caaf",
      "ff72a1a3e4c84e45adfd1c862e0d0366",
      "74aa76e6baf9468eadf8ad424de6bfa5",
      "e9c0e0f397974af8b60b16974d1886e6",
      "4b8e82efaee14b81902c6cd399133ffb",
      "aca8f5f14a6d4b978476e54c56ec82ad",
      "95a045ae90ab4baa9e5352739b9001ef",
      "18eeaf18bdde41ce990d11f6bf193501",
      "c8ac50e62e784b889d09339664904c71",
      "91568f848a8646b096bbe646c31dd519",
      "2e66f0841d754921a8fc96a7069edd10",
      "ba0c5f15983248e4816d6de0604106dd",
      "7370caf8526c451191815f4920a045d8",
      "f41416ca846747bb95f935cc6f3e3bf5",
      "f94960021e514ea8a40dcc12c9d80864"
     ]
    },
    "id": "2RbmymU2kx2V",
    "outputId": "41ce48bc-b74d-46e4-f23a-1273198c4b41"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# Load the SamSum dataset\n",
    "dataset = load_dataset(\"samsum\")\n",
    "\n",
    "# Load RoBERTa tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
    "\n",
    "# Preprocess the dataset: Tokenizing the data and adding labels\n",
    "def preprocess_function(examples):\n",
    "    inputs = tokenizer(examples[\"dialogue\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "    # Example criterion: label 1 if dialogue length > 100 words, else label 0\n",
    "    inputs[\"labels\"] = [1 if len(dialogue.split()) > 100 else 0 for dialogue in examples[\"dialogue\"]]\n",
    "    return inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Configuration for LoRA fine-tuning\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,  # Sequence classification task\n",
    "    r=16,  # Rank of the update matrices\n",
    "    lora_alpha=32,  # Alpha scaling factor\n",
    "    lora_dropout=0.1,  # Dropout rate for LoRA\n",
    "    bias=\"none\"  # Bias strategy for LoRA\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "lora_model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    save_steps=500\n",
    ")\n",
    "\n",
    "# Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Fine-tuning the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "trainer.save_model(\"./finetuned_roberta_samsum_lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TiO89EAGk_EO",
    "outputId": "74366f10-6db7-4f2a-ab47-a84b936b1cae"
   },
   "outputs": [],
   "source": [
    "!zip -r /content/roberta_ft.zip /content/finetuned_roberta_samsum_lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "gITRRvpq300W",
    "outputId": "6e02c23c-16fc-4703-9586-0ca962d3c5a9"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download(\"/content/roberta_ft.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zpWyUB9f4Y2b",
    "outputId": "baa9514e-dbb4-4a07-886a-80cdd2f00d26"
   },
   "outputs": [],
   "source": [
    "pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423,
     "referenced_widgets": [
      "947bbd30337a4822a7a9826723a9646d",
      "cb9344baa6654836a17b641311cb3d2f",
      "e9a36e4de36c4e289dc790d541680128",
      "98381b1f3da541279319b18d61f2e8d3",
      "54c27ed9d2cf44d1a17daf3f8fd6c689",
      "f742029e3ded4c548c4b06ac50feb2a3",
      "704ab4387ecf4716b34d45b636e21828",
      "d1987dbc4c4d441e8f72fa311c31e094",
      "e754414389ed440d97d7eebbb3ffbe99",
      "81fc5c93bba7462cbb0346ad7c6abbbf",
      "d67e1346ca45461fac37fcdf897e4b40",
      "ae333c3f8b6f45778d5f39664ced0e59",
      "ada0971bcb9d4b66aea92e1164d8c0b8",
      "c0a8e450922049f1bcd2e9fa95c94366",
      "3509693ba37549e398c477ee89780094",
      "8a1c690d02c0482183aa48eeccb2093f",
      "0f48b0310d1a4816982cbca2cd047a4c",
      "80f8216e512e4d25a43c83fc3b888703",
      "a40e5cb879414e7cb32beb0299cb6fa6",
      "5a17acdef6eb49f1bb3130eb51fd6e8d",
      "bdf2074f050743ccbcef99ad124544e7",
      "f13ff330e243474a803bd2cb9910ba22",
      "0eefa9ce7fa14dd69731b8106483fad8",
      "50d4af51241c45ae99912987df90bfea",
      "b1630242647643b0b5ae81ae678bee39",
      "ec7cfdb80aea4dd1a3d3652e33ae90d0",
      "01c554113e1245f7a8902d687e80ffbd",
      "ba42282622dd4d24be56bd9d6f86a4a5",
      "634d24587d2e42a8af3e40044351278f",
      "e9a24625dceb49a18f92b09f0dbef947",
      "76537a7a95634e5a8a7f6942a3f3edb7",
      "60be95baf52f4855b0c9b5a72e25925a",
      "0f5942fc9ff74e76b2a5c4b8d026cee5",
      "e520cd902d8649a48094b9228840cfee",
      "4d1fc1ab7571485d8c30d8e308b033f7",
      "7c5835f87448439290eebf9d90c14a48",
      "afb0704ea2644af491f52c8582436a53",
      "95f480173f6248628514b7bc5d75d722",
      "984e16945afc4f7bb8a2ab4fb078b617",
      "1657c2c8546844e4ab2bb84d6e90089e",
      "ac8fc60fcbb8447f889e4af49b9eb9ac",
      "a2162f6d19cd4b18aafb11f3a092a4aa",
      "dfd53c745c4a471eb63e69e343d0939d",
      "b6367e18c98044cea0abccc77d6d0093",
      "716584b09eb44ca9989e84404f8d73c2",
      "b18abd72649943308ab8297672f9ea5d",
      "e2f279879f5948f9b7ac2c35b5e3f564",
      "2d764a04953b4c2594ba684048b4e97f",
      "6aba90ffaf4744be9d7b8d358bd4ea09",
      "b200d3d433a4400aa54f692af0914053",
      "afab6966bcc44cda82947533403430be",
      "e8fdfe3045604cf4b44a78378432b7d7",
      "f78415d525eb4ab6aecf3b23cc7d1625",
      "a552ec356e4d440b89ffc5342f02eb8e",
      "619cae674078448aa33133a906ac4da1"
     ]
    },
    "id": "YlsfrUDo384a",
    "outputId": "b8a9d892-070c-41f7-821c-e678b58cfd33"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from peft import PeftModel\n",
    "import evaluate\n",
    "\n",
    "# Load the SamSum dataset\n",
    "dataset = load_dataset(\"samsum\")\n",
    "\n",
    "# Preprocess the dataset (ensure it's the same preprocessing function used before)\n",
    "def preprocess_function(examples):\n",
    "    inputs = tokenizer(examples[\"dialogue\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "    inputs[\"labels\"] = [1 if len(dialogue.split()) > 100 else 0 for dialogue in examples[\"dialogue\"]]\n",
    "    return inputs\n",
    "\n",
    "# Load the RoBERTa tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# Tokenize the validation dataset\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "validation_dataset = tokenized_datasets[\"validation\"]\n",
    "\n",
    "# Evaluation metrics\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "# Helper function to evaluate a model\n",
    "def evaluate_model(model, tokenizer, dataset):\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    # Get predictions\n",
    "    predictions = trainer.predict(dataset)\n",
    "\n",
    "    # Extract predicted labels and true labels\n",
    "    preds = torch.argmax(torch.tensor(predictions.predictions), dim=-1)\n",
    "    labels = torch.tensor(predictions.label_ids)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_metric.compute(predictions=preds, references=labels)\n",
    "    f1_score = f1_metric.compute(predictions=preds, references=labels, average=\"weighted\")\n",
    "\n",
    "    return accuracy, f1_score\n",
    "\n",
    "# -----------------------------------------------\n",
    "# 1. Evaluate the base RoBERTa model\n",
    "# -----------------------------------------------\n",
    "print(\"Evaluating Base RoBERTa Model...\")\n",
    "\n",
    "# Load the base RoBERTa model\n",
    "base_model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
    "\n",
    "# Evaluate the base model\n",
    "base_accuracy, base_f1 = evaluate_model(base_model, tokenizer, validation_dataset)\n",
    "\n",
    "print(f\"Base Model Accuracy: {base_accuracy['accuracy']:.4f}\")\n",
    "print(f\"Base Model F1 Score: {base_f1['f1']:.4f}\")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# 2. Evaluate the Fine-Tuned RoBERTa model\n",
    "# -----------------------------------------------\n",
    "print(\"\\nEvaluating Fine-Tuned RoBERTa Model...\")\n",
    "\n",
    "# Load the fine-tuned RoBERTa model with LoRA\n",
    "fine_tuned_model = RobertaForSequenceClassification.from_pretrained(\"./finetuned_roberta_samsum_lora\")\n",
    "fine_tuned_model = PeftModel.from_pretrained(fine_tuned_model, \"./finetuned_roberta_samsum_lora\")\n",
    "\n",
    "# Evaluate the fine-tuned model\n",
    "fine_tuned_accuracy, fine_tuned_f1 = evaluate_model(fine_tuned_model, tokenizer, validation_dataset)\n",
    "\n",
    "print(f\"Fine-Tuned Model Accuracy: {fine_tuned_accuracy['accuracy']:.4f}\")\n",
    "print(f\"Fine-Tuned Model F1 Score: {fine_tuned_f1['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "id": "vaJEEJtO4KoZ",
    "outputId": "77a3425e-339f-4e38-faf7-a09a8d15ba74"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer\n",
    "from peft import PeftModel\n",
    "import evaluate\n",
    "\n",
    "# Load the pruned model with LoRA (replace with your model path)\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"./finetuned_roberta_samsum_lora\")\n",
    "model = PeftModel.from_pretrained(model, \"./finetuned_roberta_samsum_lora\")\n",
    "\n",
    "# Function to prune LoRA weights (as defined before)\n",
    "def prune_lora_weights(model, percentage=0.5):\n",
    "    for n, m in model.named_modules():\n",
    "        if isinstance(m, torch.nn.Linear) and \"lora\" in n:\n",
    "            weight = m.weight.data\n",
    "            num_weights_to_prune = int(weight.numel() * percentage)\n",
    "            # _, indices = torch.topk(torch.abs(weight).view(-1), num_weights_to_prune, largest=False)\n",
    "            indices = torch.randperm(weight.numel())[:num_weights_to_prune]\n",
    "            weight.view(-1)[indices] = 0\n",
    "\n",
    "# Prune the model weights (e.g., 50% sparsity)\n",
    "prune_lora_weights(model, percentage=0.5)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# Load the Samsum dataset\n",
    "dataset = load_dataset(\"samsum\")\n",
    "\n",
    "# Preprocess the dataset (ensure it's the same preprocessing function used before)\n",
    "def preprocess_function(examples):\n",
    "    inputs = tokenizer(examples[\"dialogue\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "    inputs[\"labels\"] = [1 if len(dialogue.split()) > 100 else 0 for dialogue in examples[\"dialogue\"]]\n",
    "    return inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "validation_dataset = tokenized_datasets[\"validation\"]\n",
    "\n",
    "# Evaluation metrics\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "# Helper function to evaluate a model (same as before)\n",
    "def evaluate_model(model, tokenizer, dataset):\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    predictions = trainer.predict(dataset)\n",
    "    preds = torch.argmax(torch.tensor(predictions.predictions), dim=-1)\n",
    "    labels = torch.tensor(predictions.label_ids)\n",
    "    accuracy = accuracy_metric.compute(predictions=preds, references=labels)\n",
    "    f1_score = f1_metric.compute(predictions=preds, references=labels, average=\"weighted\")\n",
    "    return accuracy, f1_score\n",
    "\n",
    "# Evaluate the pruned model\n",
    "pruned_accuracy, pruned_f1 = evaluate_model(model, tokenizer, validation_dataset)\n",
    "\n",
    "print(f\"Pruned Model Accuracy: {pruned_accuracy['accuracy']:.4f}\")\n",
    "print(f\"Pruned Model F1 Score: {pruned_f1['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 721
    },
    "id": "-D8F9F2t6p0j",
    "outputId": "9a10eacc-86b6-4589-b67e-9da5c31e88a9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer\n",
    "from peft import PeftModel\n",
    "import evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the fine-tuned model with LoRA\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"./finetuned_roberta_samsum_lora\")\n",
    "model = PeftModel.from_pretrained(model, \"./finetuned_roberta_samsum_lora\")\n",
    "\n",
    "# Function to prune LoRA weights randomly\n",
    "def prune_lora_weights_randomly(model, percentage=0.5):\n",
    "    for n, m in model.named_modules():\n",
    "        if isinstance(m, torch.nn.Linear) and \"lora\" in n:\n",
    "            weight = m.weight.data\n",
    "            num_weights_to_prune = int(weight.numel() * percentage)\n",
    "            indices = torch.randperm(weight.numel())[:num_weights_to_prune]\n",
    "            weight.view(-1)[indices] = 0\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# Load the Samsum dataset\n",
    "dataset = load_dataset(\"samsum\")\n",
    "\n",
    "# Preprocess the dataset\n",
    "def preprocess_function(examples):\n",
    "    inputs = tokenizer(examples[\"dialogue\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "    inputs[\"labels\"] = [1 if len(dialogue.split()) > 100 else 0 for dialogue in examples[\"dialogue\"]]\n",
    "    return inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "validation_dataset = tokenized_datasets[\"validation\"]\n",
    "\n",
    "# Evaluation metrics\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "# Helper function to evaluate a model\n",
    "def evaluate_model(model, tokenizer, dataset):\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    predictions = trainer.predict(dataset)\n",
    "    preds = torch.argmax(torch.tensor(predictions.predictions), dim=-1)\n",
    "    labels = torch.tensor(predictions.label_ids)\n",
    "    accuracy = accuracy_metric.compute(predictions=preds, references=labels)\n",
    "    f1_score = f1_metric.compute(predictions=preds, references=labels, average=\"weighted\")\n",
    "    return accuracy, f1_score\n",
    "\n",
    "# Store results for plotting\n",
    "accuracies = []\n",
    "f1_scores = []\n",
    "\n",
    "# Pruning percentages to test\n",
    "pruning_percentages = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "for percentage in pruning_percentages:\n",
    "    # Create a copy of the model\n",
    "    pruned_model = copy.deepcopy(model)\n",
    "\n",
    "    # Prune the copied model weights\n",
    "    prune_lora_weights_randomly(pruned_model, percentage)\n",
    "\n",
    "    # Evaluate the pruned model\n",
    "    pruned_accuracy, pruned_f1 = evaluate_model(pruned_model, tokenizer, validation_dataset)\n",
    "\n",
    "    print(f\"Pruned Model ({percentage*100:.0f}% sparsity) - Accuracy: {pruned_accuracy['accuracy']:.4f}, F1: {pruned_f1['f1']:.4f}\")\n",
    "\n",
    "    accuracies.append(pruned_accuracy['accuracy'])\n",
    "    f1_scores.append(pruned_f1['f1'])\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(pruning_percentages, accuracies, marker='o', label=\"Accuracy\")\n",
    "plt.plot(pruning_percentages, f1_scores, marker='x', label=\"F1 Score\")\n",
    "plt.xlabel(\"Pruning Percentage\")\n",
    "plt.ylabel(\"Performance\")\n",
    "plt.title(\"Model Performance vs. Pruning Percentage\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 738
    },
    "id": "lK1yCJrj8Tg9",
    "outputId": "c9f44967-b1cf-4ab8-c13f-d30401c1c99b"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer\n",
    "from peft import PeftModel\n",
    "import evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the fine-tuned model with LoRA\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"./finetuned_roberta_samsum_lora\")\n",
    "model = PeftModel.from_pretrained(model, \"./finetuned_roberta_samsum_lora\")\n",
    "\n",
    "# Calculate the number of fine-tuning parameters\n",
    "total_lora_params = sum(p.numel() for n, p in model.named_parameters() if 'lora' in n)\n",
    "print(f\"Total fine-tuning parameters: {total_lora_params}\")\n",
    "\n",
    "# Function to prune LoRA weights randomly\n",
    "def prune_lora_weights_randomly(model, percentage=0.5):\n",
    "    for n, m in model.named_modules():\n",
    "        if isinstance(m, torch.nn.Linear) and \"lora\" in n:\n",
    "            weight = m.weight.data\n",
    "            num_weights_to_prune = int(weight.numel() * percentage)\n",
    "            _, indices = torch.topk(torch.abs(weight).view(-1), num_weights_to_prune, largest=False)\n",
    "            weight.view(-1)[indices] = 0\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# Load the Samsum dataset\n",
    "dataset = load_dataset(\"samsum\")\n",
    "\n",
    "# Preprocess the dataset\n",
    "def preprocess_function(examples):\n",
    "    inputs = tokenizer(examples[\"dialogue\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "    inputs[\"labels\"] = [1 if len(dialogue.split()) > 100 else 0 for dialogue in examples[\"dialogue\"]]\n",
    "    return inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "validation_dataset = tokenized_datasets[\"validation\"]\n",
    "\n",
    "# Evaluation metrics\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "# Helper function to evaluate a model\n",
    "def evaluate_model(model, tokenizer, dataset):\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    predictions = trainer.predict(dataset)\n",
    "    preds = torch.argmax(torch.tensor(predictions.predictions), dim=-1)\n",
    "    labels = torch.tensor(predictions.label_ids)\n",
    "    accuracy = accuracy_metric.compute(predictions=preds, references=labels)\n",
    "    f1_score = f1_metric.compute(predictions=preds, references=labels, average=\"weighted\")\n",
    "    return accuracy, f1_score\n",
    "\n",
    "# Store results for plotting\n",
    "accuracies = []\n",
    "f1_scores = []\n",
    "\n",
    "# Pruning percentages to test\n",
    "pruning_percentages = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "for percentage in pruning_percentages:\n",
    "    # Create a copy of the model\n",
    "    pruned_model = copy.deepcopy(model)\n",
    "\n",
    "    # Prune the copied model weights\n",
    "    prune_lora_weights_randomly(pruned_model, percentage)\n",
    "\n",
    "    # Evaluate the pruned model\n",
    "    pruned_accuracy, pruned_f1 = evaluate_model(pruned_model, tokenizer, validation_dataset)\n",
    "\n",
    "    print(f\"Pruned Model ({percentage*100:.0f}% sparsity) - Accuracy: {pruned_accuracy['accuracy']:.4f}, F1: {pruned_f1['f1']:.4f}\")\n",
    "\n",
    "    accuracies.append(pruned_accuracy['accuracy'])\n",
    "    f1_scores.append(pruned_f1['f1'])\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(pruning_percentages, accuracies, marker='o', label=\"Accuracy\")\n",
    "plt.plot(pruning_percentages, f1_scores, marker='x', label=\"F1 Score\")\n",
    "plt.xlabel(\"Pruning Percentage\")\n",
    "plt.ylabel(\"Performance\")\n",
    "plt.title(\"Model Performance vs. Pruning Percentage\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KRFg8xAc9zTh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
